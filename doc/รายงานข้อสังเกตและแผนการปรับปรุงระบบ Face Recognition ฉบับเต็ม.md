# üìã ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡πÅ‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö Face Recognition ‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏ï‡πá‡∏°

## üö® **‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏û‡∏ö**

### **1. ‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏à‡∏£‡∏¥‡∏á (Critical)**
```python
# ‚ùå ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô face_recognition_service.py
async def extract_embedding(self, face_image) -> Optional[FaceEmbedding]:
    # Simulate embedding extraction
    await asyncio.sleep(0.05)
    
    # Generate fake embedding - ‡∏™‡∏∏‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤!
    embedding_vector = np.random.randn(embedding_size).astype(np.float32)
    embedding_vector = embedding_vector / np.linalg.norm(embedding_vector)
```

**‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö:**
- boss_04.jpg (‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô) ‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà 51.8% ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô >95%
- ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏î‡∏à‡∏≥‡∏ú‡∏¥‡∏î‡πÅ‡∏ó‡∏ö‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û
- ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÑ‡∏î‡πâ

### **2. Model Loading ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡∏•‡∏≠‡∏á (Critical)**
```python
# ‚ùå ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÉ‡∏ô load_model()
async def load_model(self, model_type: ModelType) -> bool:
    # Simulate model loading - ‡πÅ‡∏Ñ‡πà‡∏à‡∏≥‡∏•‡∏≠‡∏á!
    await asyncio.sleep(0.1)
    self.current_model = f"{model_type.value.lower()}_model"  # ‡πÅ‡∏Ñ‡πà string
```

**‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ:**
- `adaface_ir101.onnx` (89MB)
- `arcface_r100.onnx` (249MB)  
- `facenet_vggface2.onnx` (249MB)

---

## üîç **‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏£‡∏≠‡∏á (High Priority)**

### **3. Detection Bounding Box ‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥**
```json
// ‚ùå bbox ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û - ‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
"boss_01.jpg": {
  "bbox": {"x1": 0.0, "y1": 0.0, "x2": 2544.0, "y2": 3392.0}
}

// ‚úÖ bbox ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
"boss_03.jpg": {
  "bbox": {"x1": 528.08, "y1": 1377.01, "x2": 1876.87, "y2": 3054.22}
}
```

### **4. Quality Score ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏¥‡∏î**
```json
// ‚ùå Quality score ‡πÄ‡∏Å‡∏¥‡∏ô 100
"quality_score": 789.0364074707031  // ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô 0-100

// ‚úÖ Quality score ‡∏õ‡∏Å‡∏ï‡∏¥  
"quality_score": 95.15851211547852
```

### **5. ‡πÑ‡∏°‡πà‡∏°‡∏µ Unknown Detection**
- ‡∏ó‡∏∏‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÄ‡∏õ‡πá‡∏ô "boss" ‡∏´‡∏£‡∏∑‡∏≠ "night" ‡πÄ‡∏™‡∏°‡∏≠
- ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏õ‡∏è‡∏¥‡πÄ‡∏™‡∏ò‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
- Threshold ‡∏ï‡πà‡∏≥‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (0.5) ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô 0.7-0.8

---

## üõ†Ô∏è **‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**

### **Phase 1: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á (Week 1)**

#### **1.1 ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Face Recognition Service ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î**
"""
Fixed Face Recognition Service - ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• ONNX ‡∏à‡∏£‡∏¥‡∏á
‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà fake embedding ‡∏î‡πâ‡∏ß‡∏¢ real model inference
"""

import os
import time
import logging
import numpy as np
import cv2
import onnxruntime as ort
from typing import Optional, Dict, Any, List, Tuple
import asyncio
from dataclasses import dataclass

from .models import (
    FaceEmbedding, FaceMatch, FaceRecognitionResult, 
    FaceComparisonResult, ModelType
)

logger = logging.getLogger(__name__)


@dataclass
class RecognitionConfig:
    """Updated Configuration with Unknown Detection"""
    similarity_threshold: float = 0.75      # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.6
    unknown_threshold: float = 0.70         # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
    max_faces: int = 10
    quality_threshold: float = 0.5          # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.3
    auto_model_selection: bool = True
    preferred_model: Optional[ModelType] = ModelType.FACENET
    enable_quality_assessment: bool = True
    enable_unknown_detection: bool = True   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà


class FaceRecognitionService:
    """Real Face Recognition Service with ONNX Models"""
    
    def __init__(self, config: Optional[RecognitionConfig] = None, vram_manager=None):
        self.logger = logging.getLogger(__name__)
        self.config = config or RecognitionConfig()
        self.vram_manager = vram_manager
        
        # Real ONNX Models
        self.onnx_models: Dict[ModelType, ort.InferenceSession] = {}
        self.current_model_type: Optional[ModelType] = None
        
        # Model Paths
        self.model_paths = {
            ModelType.ADAFACE: "model/face-recognition/adaface_ir101.onnx",
            ModelType.ARCFACE: "model/face-recognition/arcface_r100.onnx", 
            ModelType.FACENET: "model/face-recognition/facenet_vggface2.onnx"
        }
        
        # Model Specifications
        self.model_specs = {
            ModelType.ADAFACE: {
                'input_size': (112, 112),
                'embedding_size': 512,
                'mean': [0.5, 0.5, 0.5],
                'std': [0.5, 0.5, 0.5]
            },
            ModelType.ARCFACE: {
                'input_size': (112, 112), 
                'embedding_size': 512,
                'mean': [0.5, 0.5, 0.5],
                'std': [0.5, 0.5, 0.5]
            },
            ModelType.FACENET: {
                'input_size': (160, 160),
                'embedding_size': 512,
                'mean': [127.5, 127.5, 127.5],
                'std': [128.0, 128.0, 128.0]
            }
        }
        
        # Face Database with Multiple Embeddings
        self.face_database: Dict[str, List[FaceEmbedding]] = {}
        
        # Performance Statistics
        self.stats = {
            'total_extractions': 0,
            'total_comparisons': 0,
            'total_recognitions': 0,
            'successful_recognitions': 0,
            'unknown_detections': 0,
            'processing_times': []
        }
        
        self.logger.info("Real Face Recognition Service initialized")
    
    async def initialize(self) -> bool:
        """Initialize with real ONNX model loading"""
        try:
            # Select model
            if self.config.auto_model_selection:
                model_type = await self._select_best_model()
            else:
                model_type = self.config.preferred_model or ModelType.FACENET
            
            # Load real ONNX model
            success = await self.load_model(model_type)
            
            if success:
                self.logger.info(f"‚úÖ Service initialized with REAL {model_type.value} model")
                return True
            else:
                self.logger.error("‚ùå Failed to initialize service")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå Error initializing service: {e}")
            return False
    
    async def load_model(self, model_type: ModelType) -> bool:
        """Load real ONNX model (not simulation!)"""
        try:
            model_path = self.model_paths[model_type]
            
            # Check if model file exists
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"ONNX model not found: {model_path}")
            
            self.logger.info(f"üîÑ Loading REAL ONNX model: {model_path}")
            
            # Configure ONNX Runtime providers
            providers = []
            if self.vram_manager:
                # Use GPU if available
                providers.extend(['CUDAExecutionProvider', 'CPUExecutionProvider'])
            else:
                providers.append('CPUExecutionProvider')
            
            # Session options for optimization
            session_options = ort.SessionOptions()
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # Load real ONNX model
            self.onnx_models[model_type] = ort.InferenceSession(
                model_path,
                providers=providers,
                sess_options=session_options
            )
            
            self.current_model_type = model_type
            
            # Verify model inputs/outputs
            model = self.onnx_models[model_type]
            input_info = model.get_inputs()[0]
            output_info = model.get_outputs()[0]
            
            self.logger.info(f"‚úÖ REAL model loaded successfully!")
            self.logger.info(f"   Input shape: {input_info.shape}")
            self.logger.info(f"   Output shape: {output_info.shape}")
            self.logger.info(f"   Providers: {model.get_providers()}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load REAL model {model_type.value}: {e}")
            return False
    
    async def extract_embedding(self, face_image: np.ndarray) -> Optional[FaceEmbedding]:
        """Extract REAL embedding using ONNX model"""
        if self.current_model_type not in self.onnx_models:
            self.logger.error("‚ùå No model loaded!")
            return None
        
        start_time = time.time()
        
        try:
            # Get model and specifications
            model = self.onnx_models[self.current_model_type]
            specs = self.model_specs[self.current_model_type]
            
            # Preprocess image for specific model
            input_tensor = self._preprocess_image(face_image, self.current_model_type)
            
            # Run REAL model inference
            input_name = model.get_inputs()[0].name
            outputs = model.run(None, {input_name: input_tensor})
            
            # Extract embedding from output
            embedding_vector = outputs[0][0]  # Shape: (embedding_size,)
            
            # Normalize embedding
            embedding_vector = embedding_vector / (np.linalg.norm(embedding_vector) + 1e-8)
            
            # Calculate quality score
            quality_score = self._calculate_embedding_quality(embedding_vector)
            
            processing_time = time.time() - start_time
            
            # Update statistics
            self.stats['total_extractions'] += 1
            self.stats['processing_times'].append(processing_time)
            
            embedding = FaceEmbedding(
                vector=embedding_vector.astype(np.float32),
                model_type=self.current_model_type,
                quality_score=quality_score,
                extraction_time=processing_time
            )
            
            self.logger.debug(f"‚úÖ REAL embedding extracted: {embedding_vector.shape}, quality: {quality_score:.3f}")
            
            return embedding
            
        except Exception as e:
            self.logger.error(f"‚ùå REAL embedding extraction failed: {e}")
            return None
    
    def _preprocess_image(self, face_image: np.ndarray, model_type: ModelType) -> np.ndarray:
        """Preprocess image for specific model"""
        specs = self.model_specs[model_type]
        input_size = specs['input_size']
        
        try:
            # Resize to model input size
            if face_image.shape[:2] != input_size:
                face_resized = cv2.resize(face_image, input_size, interpolation=cv2.INTER_AREA)
            else:
                face_resized = face_image.copy()
            
            # Convert to float32
            face_float = face_resized.astype(np.float32)
            
            # Model-specific normalization
            if model_type == ModelType.FACENET:
                # FaceNet: [-1, 1]
                face_normalized = (face_float - 127.5) / 128.0
            else:
                # ArcFace/AdaFace: [0, 1] -> [-1, 1]
                face_normalized = (face_float / 255.0 - 0.5) / 0.5
            
            # Convert from HWC to CHW and add batch dimension
            input_tensor = np.transpose(face_normalized, (2, 0, 1))
            input_tensor = np.expand_dims(input_tensor, axis=0)
            
            return input_tensor
            
        except Exception as e:
            self.logger.error(f"‚ùå Image preprocessing failed: {e}")
            raise
    
    def _calculate_embedding_quality(self, embedding: np.ndarray) -> float:
        """Calculate embedding quality score (0-1)"""
        try:
            # Calculate embedding statistics
            magnitude = np.linalg.norm(embedding)
            variance = np.var(embedding)
            sparsity = np.sum(np.abs(embedding) < 0.01) / len(embedding)
            
            # Quality metrics
            magnitude_score = min(magnitude, 1.0)  # Good embeddings have ~1.0 magnitude
            variance_score = min(variance * 10, 1.0)  # Higher variance = more informative
            sparsity_score = max(0, 1.0 - sparsity)  # Lower sparsity = better
            
            # Combined quality score
            quality = (magnitude_score * 0.4 + variance_score * 0.4 + sparsity_score * 0.2)
            
            return float(np.clip(quality, 0.0, 1.0))
            
        except Exception:
            return 0.5  # Default medium quality
    
    async def recognize_face(self, face_image: np.ndarray) -> FaceRecognitionResult:
        """Recognize face with Unknown Detection"""
        start_time = time.time()
        
        try:
            # Extract embedding
            embedding = await self.extract_embedding(face_image)
            if embedding is None:
                return self._create_failed_result(start_time, "Failed to extract embedding")
            
            # Search in database
            matches = []
            for person_id, stored_embeddings in self.face_database.items():
                best_similarity = 0.0
                best_embedding = None
                
                # Compare with all embeddings of this person
                for stored_embedding in stored_embeddings:
                    if stored_embedding.model_type != embedding.model_type:
                        continue
                    
                    similarity = self._cosine_similarity(
                        embedding.vector, stored_embedding.vector
                    )
                    
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_embedding = stored_embedding
                
                # Create match if above threshold
                if best_similarity >= self.config.similarity_threshold:
                    match = FaceMatch(
                        person_id=person_id,
                        confidence=best_similarity,
                        embedding=best_embedding
                    )
                    matches.append(match)
            
            # Sort by confidence
            matches.sort(key=lambda x: x.confidence, reverse=True)
            
            # Unknown Detection Logic
            best_match = None
            if matches:
                candidate = matches[0]
                
                # Check against unknown threshold
                if (self.config.enable_unknown_detection and 
                    candidate.confidence < self.config.unknown_threshold):
                    # Mark as unknown
                    self.stats['unknown_detections'] += 1
                    self.logger.info(f"üîç Unknown face detected (confidence: {candidate.confidence:.3f} < {self.config.unknown_threshold:.3f})")
                else:
                    best_match = candidate
                    self.stats['successful_recognitions'] += 1
                    self.logger.info(f"‚úÖ Face recognized: {best_match.person_id} (confidence: {best_match.confidence:.3f})")
            else:
                self.stats['unknown_detections'] += 1
                self.logger.info("üîç No matches found - Unknown face")
            
            # Update statistics
            self.stats['total_recognitions'] += 1
            processing_time = time.time() - start_time
            
            return FaceRecognitionResult(
                matches=matches,
                best_match=best_match,
                confidence=best_match.confidence if best_match else 0.0,
                processing_time=processing_time,
                model_used=self.current_model_type,
                face_embedding=embedding  # Include the extracted embedding
            )
            
        except Exception as e:
            self.logger.error(f"‚ùå Face recognition failed: {e}")
            return self._create_failed_result(start_time, str(e))
    
    def _cosine_similarity(self, embedding1: np.ndarray, embedding2: np.ndarray) -> float:
        """Calculate cosine similarity between embeddings"""
        try:
            # Ensure embeddings are normalized
            norm1 = np.linalg.norm(embedding1)
            norm2 = np.linalg.norm(embedding2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            # Cosine similarity
            similarity = np.dot(embedding1, embedding2) / (norm1 * norm2)
            
            # Convert to 0-1 range (from -1 to 1)
            similarity = (similarity + 1.0) / 2.0
            
            return float(np.clip(similarity, 0.0, 1.0))
            
        except Exception as e:
            self.logger.error(f"‚ùå Similarity calculation failed: {e}")
            return 0.0
    
    async def add_face_to_database(self, person_id: str, face_image: np.ndarray, 
                                  metadata: Optional[Dict[str, Any]] = None) -> bool:
        """Add face to database with multiple embeddings support"""
        try:
            embedding = await self.extract_embedding(face_image)
            if embedding is None:
                self.logger.error(f"‚ùå Failed to extract embedding for {person_id}")
                return False
            
            # Add metadata
            if metadata:
                embedding.metadata = metadata
            
            # Add to database (support multiple embeddings per person)
            if person_id not in self.face_database:
                self.face_database[person_id] = []
            
            self.face_database[person_id].append(embedding)
            
            total_embeddings = len(self.face_database[person_id])
            self.logger.info(f"‚úÖ Added embedding for {person_id} (total: {total_embeddings})")
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to add face to database: {e}")
            return False
    
    def _create_failed_result(self, start_time: float, error_message: str) -> FaceRecognitionResult:
        """Create failed recognition result"""
        return FaceRecognitionResult(
            matches=[],
            best_match=None,
            confidence=0.0,
            processing_time=time.time() - start_time,
            model_used=self.current_model_type,
            error=error_message
        )
    
    async def _select_best_model(self) -> ModelType:
        """Select best model based on available resources"""
        # For now, prefer FaceNet for balance of speed and accuracy
        # TODO: Integrate with VRAM manager for intelligent selection
        return ModelType.FACENET
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics"""
        try:
            processing_times = self.stats['processing_times']
            avg_time = np.mean(processing_times) if processing_times else 0.0
            
            recognition_rate = (
                self.stats['successful_recognitions'] / self.stats['total_recognitions']
                if self.stats['total_recognitions'] > 0 else 0.0
            )
            
            unknown_rate = (
                self.stats['unknown_detections'] / self.stats['total_recognitions']  
                if self.stats['total_recognitions'] > 0 else 0.0
            )
            
            return {
                'current_model': self.current_model_type.value if self.current_model_type else None,
                'model_loaded': self.current_model_type is not None,
                'database_size': len(self.face_database),
                'total_embeddings': sum(len(embs) for embs in self.face_database.values()),
                'performance': {
                    'total_extractions': self.stats['total_extractions'],
                    'total_recognitions': self.stats['total_recognitions'],
                    'successful_recognitions': self.stats['successful_recognitions'],
                    'unknown_detections': self.stats['unknown_detections'],
                    'recognition_rate': recognition_rate,
                    'unknown_detection_rate': unknown_rate,
                    'average_processing_time': avg_time
                },
                'thresholds': {
                    'similarity_threshold': self.config.similarity_threshold,
                    'unknown_threshold': self.config.unknown_threshold,
                    'quality_threshold': self.config.quality_threshold
                }
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to get performance stats: {e}")
            return {'error': str(e)}
    
    async def cleanup(self):
        """Cleanup resources"""
        try:
            # Clear ONNX models
            for model in self.onnx_models.values():
                del model
            self.onnx_models.clear()
            
            # Clear database
            self.face_database.clear()
            
            # Reset state
            self.current_model_type = None
            
            self.logger.info("‚úÖ Face Recognition Service cleaned up")
            
        except Exception as e:
            self.logger.error(f"‚ùå Cleanup failed: {e}")

#### **1.2 ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Detection Bounding Box Issues**
"""
Fixed Face Detection Utils
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Quality Score ‡πÅ‡∏•‡∏∞ Bounding Box ‡∏ó‡∏µ‡πà‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
"""

import cv2
import numpy as np
from typing import Tuple, Dict, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


def calculate_face_quality(detection_bbox, image_shape: Tuple[int, int]) -> float:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 0-100
    
    Args:
        detection_bbox: BoundingBox object ‡∏´‡∏£‡∏∑‡∏≠ dict
        image_shape: (height, width) ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
    
    Returns:
        ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û 0-100 (‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 100)
    """
    try:
        # Extract coordinates
        if hasattr(detection_bbox, 'x1'):
            # BoundingBox object
            x1, y1, x2, y2 = detection_bbox.x1, detection_bbox.y1, detection_bbox.x2, detection_bbox.y2
            confidence = detection_bbox.confidence
        else:
            # Dict format
            x1 = detection_bbox.get('x1', 0)
            y1 = detection_bbox.get('y1', 0) 
            x2 = detection_bbox.get('x2', 0)
            y2 = detection_bbox.get('y2', 0)
            confidence = detection_bbox.get('confidence', 0)
        
        # Image dimensions
        img_height, img_width = image_shape[:2]
        
        # Face dimensions
        face_width = max(x2 - x1, 1)
        face_height = max(y2 - y1, 1)
        face_area = face_width * face_height
        
        # Image area
        image_area = img_width * img_height
        
        # ===== ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏¢‡πà‡∏≠‡∏¢ =====
        
        # 1. Size Score (40%) - ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        area_ratio = face_area / image_area
        if area_ratio > 0.1:  # ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å (>10% ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ)
            size_score = 100
        elif area_ratio > 0.05:  # ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏´‡∏ç‡πà (5-10%)
            size_score = 90
        elif area_ratio > 0.02:  # ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á (2-5%)  
            size_score = 75
        elif area_ratio > 0.005:  # ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏•‡πá‡∏Å (0.5-2%)
            size_score = 50
        else:  # ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏•‡πá‡∏Å‡∏°‡∏≤‡∏Å (<0.5%)
            size_score = 25
        
        # 2. Resolution Score (30%) - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        min_face_dimension = min(face_width, face_height)
        if min_face_dimension >= 200:  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á
            resolution_score = 100
        elif min_face_dimension >= 100:  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏î‡∏µ
            resolution_score = 85
        elif min_face_dimension >= 64:   # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏û‡∏≠‡πÉ‡∏ä‡πâ
            resolution_score = 70
        elif min_face_dimension >= 32:   # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡πà‡∏≥
            resolution_score = 50
        else:  # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å
            resolution_score = 20
        
        # 3. Confidence Score (20%) - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏Ç‡∏≠‡∏á detection
        confidence_score = min(confidence * 100, 100)  # Convert to 0-100
        
        # 4. Aspect Ratio Score (10%) - ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        aspect_ratio = face_width / face_height
        ideal_ratio = 0.8  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏õ‡∏Å‡∏ï‡∏¥
        
        ratio_diff = abs(aspect_ratio - ideal_ratio)
        if ratio_diff < 0.1:  # ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏î‡∏µ‡∏°‡∏≤‡∏Å
            aspect_score = 100
        elif ratio_diff < 0.2:  # ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏î‡∏µ
            aspect_score = 80
        elif ratio_diff < 0.3:  # ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏û‡∏≠‡πÉ‡∏ä‡πâ
            aspect_score = 60
        else:  # ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
            aspect_score = 30
        
        # ===== ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏£‡∏ß‡∏° =====
        final_score = (
            size_score * 0.40 +           # 40% ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏ô‡∏≤‡∏î
            resolution_score * 0.30 +     # 30% ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
            confidence_score * 0.20 +     # 20% ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å confidence
            aspect_score * 0.10           # 10% ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô
        )
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-100
        final_score = max(0.0, min(100.0, final_score))
        
        # Log debug info
        logger.debug(f"Quality calculation: area_ratio={area_ratio:.4f}, "
                    f"resolution={min_face_dimension}px, confidence={confidence:.3f}, "
                    f"aspect_ratio={aspect_ratio:.3f}, final_score={final_score:.1f}")
        
        return float(final_score)
        
    except Exception as e:
        logger.error(f"‚ùå Quality calculation failed: {e}")
        return 50.0  # Default medium quality


def validate_bounding_box(bbox, image_shape: Tuple[int, int]) -> bool:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á bounding box
    
    Args:
        bbox: BoundingBox object ‡∏´‡∏£‡∏∑‡∏≠ dict
        image_shape: (height, width) ‡∏Ç‡∏≠‡∏á‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        
    Returns:
        True ‡∏ñ‡πâ‡∏≤ bbox ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á, False ‡∏ñ‡πâ‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
    """
    try:
        # Extract coordinates
        if hasattr(bbox, 'x1'):
            x1, y1, x2, y2 = bbox.x1, bbox.y1, bbox.x2, bbox.y2
        else:
            x1 = bbox.get('x1', 0)
            y1 = bbox.get('y1', 0)
            x2 = bbox.get('x2', 0) 
            y2 = bbox.get('y2', 0)
        
        img_height, img_width = image_shape[:2]
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        if x1 >= x2 or y1 >= y2:
            logger.warning(f"‚ùå Invalid bbox: x1={x1}, y1={y1}, x2={x2}, y2={y2}")
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï
        if x1 < 0 or y1 < 0 or x2 > img_width or y2 > img_height:
            logger.warning(f"‚ùå Bbox out of bounds: ({x1},{y1})-({x2},{y2}) vs image ({img_width},{img_height})")
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥
        width = x2 - x1
        height = y2 - y1
        
        if width < 16 or height < 16:
            logger.warning(f"‚ùå Bbox too small: {width}x{height}")
            return False
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö)
        area_ratio = (width * height) / (img_width * img_height)
        if area_ratio > 0.9:  # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° >90% ‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û
            logger.warning(f"‚ùå Bbox covers entire image: {area_ratio:.3f}")
            return False
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Bbox validation failed: {e}")
        return False


def filter_detection_results(faces: list, image_shape: Tuple[int, int], 
                           min_quality: float = 50.0) -> list:
    """
    ‡∏Å‡∏£‡∏≠‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå detection ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
    
    Args:
        faces: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ FaceDetection objects
        image_shape: ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        min_quality: ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ (0-100)
        
    Returns:
        ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏≠‡∏á
    """
    filtered_faces = []
    
    for face in faces:
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö bounding box
            if not validate_bounding_box(face.bbox, image_shape):
                logger.debug(f"üö´ Face filtered: invalid bbox")
                continue
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏°‡πà‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
            if face.quality_score is None or face.quality_score > 100:
                face.quality_score = calculate_face_quality(face.bbox, image_shape)
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            if face.quality_score >= min_quality:
                filtered_faces.append(face)
                logger.debug(f"‚úÖ Face accepted: quality={face.quality_score:.1f}")
            else:
                logger.debug(f"üö´ Face filtered: quality={face.quality_score:.1f} < {min_quality}")
                
        except Exception as e:
            logger.error(f"‚ùå Error filtering face: {e}")
            continue
    
    logger.info(f"üîç Face filtering: {len(faces)} -> {len(filtered_faces)} faces")
    
    return filtered_faces


def improve_detection_accuracy(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    
    Args:
        config: ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
        
    Returns:
        ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
    """
    improved_config = config.copy()
    
    # ‡∏õ‡∏£‡∏±‡∏ö detection parameters
    improved_config.update({
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° confidence threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î false positives
        'conf_threshold': max(0.4, config.get('conf_threshold', 0.15)),
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° IoU threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î duplicate detections  
        'iou_threshold': max(0.6, config.get('iou_threshold', 0.4)),
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏Ç‡∏≠‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
        'min_face_size': max(32, config.get('min_face_size', 16)),
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        'min_quality_threshold': max(60, config.get('min_quality_threshold', 50)),
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡πÇ‡∏°‡πÄ‡∏î‡∏•
        'max_usable_faces_yolov9': min(6, config.get('max_usable_faces_yolov9', 8)),
        'min_agreement_ratio': max(0.8, config.get('min_agreement_ratio', 0.7)),
    })
    
    logger.info("üîß Detection config improved:")
    for key, value in improved_config.items():
        if key in config and config[key] != value:
            logger.info(f"   {key}: {config[key]} -> {value}")
    
    return improved_config


@dataclass
class QualityAnalysisResult:
    """‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"""
    total_faces: int
    valid_faces: int
    high_quality_faces: int  # >80
    medium_quality_faces: int  # 60-80
    low_quality_faces: int  # <60
    average_quality: float
    quality_distribution: Dict[str, int]
    
    def __post_init__(self):
        self.valid_ratio = self.valid_faces / self.total_faces if self.total_faces > 0 else 0
        self.high_quality_ratio = self.high_quality_faces / self.total_faces if self.total_faces > 0 else 0


def analyze_detection_quality(faces: list, image_shape: Tuple[int, int]) -> QualityAnalysisResult:
    """
    ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£ detection ‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°
    
    Args:
        faces: ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ FaceDetection objects
        image_shape: ‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        
    Returns:
        ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
    """
    if not faces:
        return QualityAnalysisResult(
            total_faces=0, valid_faces=0, high_quality_faces=0,
            medium_quality_faces=0, low_quality_faces=0,
            average_quality=0.0, quality_distribution={}
        )
    
    valid_faces = 0
    high_quality = 0
    medium_quality = 0
    low_quality = 0
    quality_scores = []
    
    for face in faces:
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏´‡∏£‡∏∑‡∏≠‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥
        if face.quality_score is None or face.quality_score > 100:
            face.quality_score = calculate_face_quality(face.bbox, image_shape)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        if validate_bounding_box(face.bbox, image_shape):
            valid_faces += 1
            quality_scores.append(face.quality_score)
            
            # ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            if face.quality_score >= 80:
                high_quality += 1
            elif face.quality_score >= 60:
                medium_quality += 1
            else:
                low_quality += 1
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
    avg_quality = np.mean(quality_scores) if quality_scores else 0.0
    
    # ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
    quality_distribution = {
        'excellent': sum(1 for q in quality_scores if q >= 90),
        'good': sum(1 for q in quality_scores if 80 <= q < 90),
        'fair': sum(1 for q in quality_scores if 60 <= q < 80),
        'poor': sum(1 for q in quality_scores if q < 60)
    }
    
    return QualityAnalysisResult(
        total_faces=len(faces),
        valid_faces=valid_faces,
        high_quality_faces=high_quality,
        medium_quality_faces=medium_quality,
        low_quality_faces=low_quality,
        average_quality=avg_quality,
        quality_distribution=quality_distribution
    )


def create_quality_report(analysis: QualityAnalysisResult) -> str:
    """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£ detection"""
    report = f"""
üìä Face Detection Quality Report
=====================================
Total Faces Detected: {analysis.total_faces}
Valid Faces: {analysis.valid_faces} ({analysis.valid_ratio:.1%})
Average Quality: {analysis.average_quality:.1f}/100

Quality Distribution:
‚îú‚îÄ Excellent (90-100): {analysis.quality_distribution['excellent']} faces
‚îú‚îÄ Good (80-89): {analysis.quality_distribution['good']} faces  
‚îú‚îÄ Fair (60-79): {analysis.quality_distribution['fair']} faces
‚îî‚îÄ Poor (<60): {analysis.quality_distribution['poor']} faces

High Quality Ratio: {analysis.high_quality_ratio:.1%}
"""
    return report.strip()
#### **1.3 ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Face Detection Service**

"""
Improved Face Detection Service
‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤ bounding box ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
"""

import time
import logging
import cv2
import numpy as np
from typing import Dict, List, Any, Optional, Union

from .yolo_models import YOLOv9ONNXDetector, YOLOv11Detector
from .utils import (
    BoundingBox, FaceDetection, DetectionResult, 
    calculate_face_quality, validate_bounding_box,
    filter_detection_results, improve_detection_accuracy,
    analyze_detection_quality, create_quality_report
)

logger = logging.getLogger(__name__)


class ImprovedFaceDetectionService:
    """
    ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Face Detection Service ‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
    """
    
    def __init__(self, vram_manager, config: Dict[str, Any]):
        """
        Initialize with improved configuration
        """
        self.vram_manager = vram_manager
        
        # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
        self.config = improve_detection_accuracy(config)
        
        self.models: Dict[str, Union[YOLOv9ONNXDetector, YOLOv11Detector]] = {}
        self.model_stats: Dict[str, Dict[str, Any]] = {}
        
        # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        self.decision_criteria = {
            'max_usable_faces_yolov9': self.config.get('max_usable_faces_yolov9', 6),  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 8
            'min_agreement_ratio': self.config.get('min_agreement_ratio', 0.8),      # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.7
            'min_quality_threshold': self.config.get('min_quality_threshold', 60),   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 50
            'iou_threshold': self.config.get('iou_threshold', 0.6)                   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.5
        }
        
        # ‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        self.detection_params = {
            'conf_threshold': self.config.get('conf_threshold', 0.4),  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.15
            'iou_threshold': self.config.get('iou_threshold', 0.6),    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.4
            'img_size': self.config.get('img_size', 640)
        }
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
        self.performance_stats = {
            'total_detections': 0,
            'valid_detections': 0,
            'filtered_detections': 0,
            'model_usage': {'yolov9c': 0, 'yolov9e': 0, 'yolov11m': 0},
            'average_quality': 0.0,
            'processing_times': []
        }
        
        self.models_loaded = False
        
        logger.info("üîß Improved Face Detection Service initialized")
        logger.info(f"üìä Enhanced detection params: {self.detection_params}")
    
    async def initialize(self) -> bool:
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        try:
            logger.info("üöÄ Loading improved face detection models...")
            
            # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ parameters ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
            success = await self._load_all_models()
            
            if success:
                self.models_loaded = True
                logger.info("‚úÖ Improved face detection models loaded successfully")
                return True
            else:
                logger.error("‚ùå Failed to load models")
                return False
                
        except Exception as e:
            logger.error(f"‚ùå Model loading failed: {e}")
            return False
    
    async def detect_faces(self, 
                         image_input: Union[str, np.ndarray],
                         model_name: Optional[str] = None,
                         conf_threshold: Optional[float] = None,
                         iou_threshold: Optional[float] = None,
                         enhanced_mode: bool = True) -> DetectionResult:
        """
        ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        """
        if not self.models_loaded:
            raise RuntimeError("Models not loaded. Call initialize() first.")
        
        # ‡πÉ‡∏ä‡πâ parameters ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        conf_threshold = conf_threshold or self.detection_params['conf_threshold']
        iou_threshold = iou_threshold or self.detection_params['iou_threshold']
        
        start_time = time.time()
        
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
            image = self._load_image(image_input)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            if model_name in ['yolov9c', 'yolov9e', 'yolov11m']:
                detections = self._detect_with_specific_model(
                    image, model_name, conf_threshold, iou_threshold
                )
                model_used = model_name
            else:
                # ‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
                detections, model_used = await self._enhanced_intelligent_detect(
                    image, conf_threshold, iou_threshold
                )
            
            # ‡∏Å‡∏£‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            detections = self._post_process_detections(detections, image.shape)
            
            total_time = time.time() - start_time
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            self._update_performance_stats(detections, model_used, total_time)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            result = DetectionResult(
                faces=detections,
                image_shape=(image.shape[0], image.shape[1], image.shape[2] if len(image.shape) > 2 else 3),
                total_processing_time=total_time,
                model_used=model_used,
                fallback_used=False
            )
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            quality_analysis = analyze_detection_quality(detections, image.shape)
            result.quality_analysis = quality_analysis
            
            logger.info(f"‚úÖ Detection complete: {len(detections)} faces, "
                       f"avg quality: {quality_analysis.average_quality:.1f}, "
                       f"model: {model_used}, time: {total_time:.3f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Face detection failed: {e}")
            raise
    
    def _detect_with_specific_model(self, 
                                   image: np.ndarray, 
                                   model_name: str,
                                   conf_threshold: float,
                                   iou_threshold: float) -> List[FaceDetection]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ - ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á"""
        start_time = time.time()
        
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
            detections_raw = self.models[model_name].detect(
                image, conf_threshold, iou_threshold
            )
            
            inference_time = time.time() - start_time
            
            # ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            face_detections = []
            for det in detections_raw:
                bbox = BoundingBox.from_array(det)
                
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö bbox ‡πÉ‡∏´‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                if not validate_bounding_box(bbox, image.shape):
                    logger.debug(f"üö´ Invalid bbox filtered: {bbox.to_array()}")
                    continue
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
                quality_score = calculate_face_quality(bbox, image.shape)
                
                face = FaceDetection(
                    bbox=bbox,
                    quality_score=quality_score,
                    model_used=model_name,
                    processing_time=inference_time
                )
                face_detections.append(face)
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
            self.model_stats[model_name] = {
                'last_inference_time': inference_time,
                'face_count': len(face_detections),
                'valid_faces': len(face_detections),
                'avg_quality': np.mean([f.quality_score for f in face_detections]) if face_detections else 0.0
            }
            
            logger.debug(f"üîç {model_name}: {len(detections_raw)} raw -> {len(face_detections)} valid faces")
            
            return face_detections
            
        except Exception as e:
            logger.error(f"‚ùå Detection with {model_name} failed: {e}")
            return []
    
    async def _enhanced_intelligent_detect(self,
                                         image: np.ndarray,
                                         conf_threshold: float,
                                         iou_threshold: float) -> tuple[List[FaceDetection], str]:
        """
        ‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        """
        logger.debug("üß† Using enhanced intelligent detection...")
        
        # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô 1: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ YOLOv9c (‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
        yolov9c_detections = self._detect_with_specific_model(
            image, 'yolov9c', conf_threshold, iou_threshold
        )
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏•‡∏¢ ‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ YOLOv11m
        if not yolov9c_detections:
            logger.debug("üîÑ YOLOv9c found no faces, trying YOLOv11m...")
            yolov11m_detections = self._detect_with_specific_model(
                image, 'yolov11m', conf_threshold, iou_threshold
            )
            return yolov11m_detections, 'yolov11m'
        
        # ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡πâ‡∏≠‡∏¢ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ YOLOv9c ‡πÄ‡∏•‡∏¢
        max_faces = self.decision_criteria['max_usable_faces_yolov9']
        if len(yolov9c_detections) <= max_faces:
            logger.debug(f"‚úÖ YOLOv9c sufficient: {len(yolov9c_detections)} faces ‚â§ {max_faces}")
            return yolov9c_detections, 'yolov9c'
        
        # ‡∏ñ‡πâ‡∏≤‡∏û‡∏ö‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ YOLOv9e
        logger.debug(f"üîÑ YOLOv9c found many faces ({len(yolov9c_detections)}), testing YOLOv9e...")
        yolov9e_detections = self._detect_with_specific_model(
            image, 'yolov9e', conf_threshold, iou_threshold
        )
        
        # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß
        agreement = self._calculate_improved_agreement(
            yolov9c_detections, yolov9e_detections
        )
        
        min_agreement = self.decision_criteria['min_agreement_ratio']
        if agreement >= min_agreement:
            logger.debug(f"‚úÖ YOLOv9 models agree: {agreement:.1%} ‚â• {min_agreement:.1%}")
            # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
            return self._select_better_detection(yolov9c_detections, yolov9e_detections)
        
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡πÉ‡∏ä‡πâ YOLOv11m
        logger.debug(f"üîÑ YOLOv9 models disagree ({agreement:.1%}), using YOLOv11m...")
        yolov11m_detections = self._detect_with_specific_model(
            image, 'yolov11m', conf_threshold, iou_threshold
        )
        
        return yolov11m_detections, 'yolov11m'
    
    def _calculate_improved_agreement(self, 
                                    detections1: List[FaceDetection], 
                                    detections2: List[FaceDetection]) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡πá‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
        if not detections1 or not detections2:
            return 0.0
        
        # ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ detections ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ
        quality_threshold = self.decision_criteria['min_quality_threshold']
        
        good_detections1 = [d for d in detections1 if d.quality_score >= quality_threshold]
        good_detections2 = [d for d in detections2 if d.quality_score >= quality_threshold]
        
        if not good_detections1 or not good_detections2:
            return 0.0
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì overlap ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á high-quality faces
        total_faces = max(len(good_detections1), len(good_detections2))
        iou_threshold = self.decision_criteria['iou_threshold']
        
        matched_count = 0
        for face1 in good_detections1:
            for face2 in good_detections2:
                iou = self._calculate_bbox_iou(face1.bbox, face2.bbox)
                if iou >= iou_threshold:
                    matched_count += 1
                    break  # ‡∏´‡∏≤ match ‡πÅ‡∏•‡πâ‡∏ß‡∏Ç‡πâ‡∏≤‡∏°‡πÑ‡∏õ‡∏´‡∏≤‡∏Ñ‡∏π‡πà‡∏ï‡πà‡∏≠‡πÑ‡∏õ
        
        agreement = matched_count / total_faces
        logger.debug(f"ü§ù Agreement: {matched_count}/{total_faces} = {agreement:.1%}")
        
        return agreement
    
    def _select_better_detection(self, 
                               detections1: List[FaceDetection], 
                               detections2: List[FaceDetection]) -> tuple[List[FaceDetection], str]:
        """‡πÄ‡∏•‡∏∑‡∏≠‡∏Å detection ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤"""
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
        avg_quality1 = np.mean([d.quality_score for d in detections1]) if detections1 else 0
        avg_quality2 = np.mean([d.quality_score for d in detections2]) if detections2 else 0
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
        high_quality1 = sum(1 for d in detections1 if d.quality_score >= 80)
        high_quality2 = sum(1 for d in detections2 if d.quality_score >= 80)
        
        # ‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        if high_quality2 > high_quality1:
            logger.debug(f"‚úÖ YOLOv9e selected: {high_quality2} vs {high_quality1} high-quality faces")
            return detections2, 'yolov9e'
        elif high_quality1 > high_quality2:
            logger.debug(f"‚úÖ YOLOv9c selected: {high_quality1} vs {high_quality2} high-quality faces")
            return detections1, 'yolov9c'
        else:
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
            if avg_quality2 > avg_quality1:
                logger.debug(f"‚úÖ YOLOv9e selected: avg quality {avg_quality2:.1f} vs {avg_quality1:.1f}")
                return detections2, 'yolov9e'
            else:
                logger.debug(f"‚úÖ YOLOv9c selected: avg quality {avg_quality1:.1f} vs {avg_quality2:.1f}")
                return detections1, 'yolov9c'
    
    def _post_process_detections(self, 
                               detections: List[FaceDetection], 
                               image_shape: tuple) -> List[FaceDetection]:
        """‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏´‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö"""
        if not detections:
            return detections
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        min_quality = self.decision_criteria['min_quality_threshold']
        filtered_detections = filter_detection_results(detections, image_shape, min_quality)
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        filtered_detections.sort(key=lambda x: x.quality_score, reverse=True)
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        max_faces = self.config.get('max_faces', 50)
        if len(filtered_detections) > max_faces:
            logger.info(f"üî¢ Limiting faces: {len(filtered_detections)} -> {max_faces}")
            filtered_detections = filtered_detections[:max_faces]
        
        return filtered_detections
    
    def _calculate_bbox_iou(self, bbox1: BoundingBox, bbox2: BoundingBox) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì IoU ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á bounding boxes"""
        try:
            # ‡∏´‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
            x_left = max(bbox1.x1, bbox2.x1)
            y_top = max(bbox1.y1, bbox2.y1)
            x_right = min(bbox1.x2, bbox2.x2)
            y_bottom = min(bbox1.y2, bbox2.y2)
            
            if x_right < x_left or y_bottom < y_top:
                return 0.0
            
            intersection_area = (x_right - x_left) * (y_bottom - y_top)
            
            # ‡∏´‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°
            area1 = bbox1.area
            area2 = bbox2.area
            union_area = area1 + area2 - intersection_area
            
            if union_area == 0:
                return 0.0
            
            return intersection_area / union_area
            
        except Exception:
            return 0.0
    
    def _update_performance_stats(self, 
                                detections: List[FaceDetection], 
                                model_used: str, 
                                processing_time: float):
        """‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û"""
        self.performance_stats['total_detections'] += len(detections)
        self.performance_stats['model_usage'][model_used] += 1
        self.performance_stats['processing_times'].append(processing_time)
        
        if detections:
            qualities = [d.quality_score for d in detections]
            current_avg = np.mean(qualities)
            
            # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ (rolling average)
            total_calls = sum(self.performance_stats['model_usage'].values())
            if total_calls == 1:
                self.performance_stats['average_quality'] = current_avg
            else:
                self.performance_stats['average_quality'] = (
                    (self.performance_stats['average_quality'] * (total_calls - 1) + current_avg) / total_calls
                )
    
    def get_enhanced_service_info(self) -> Dict[str, Any]:
        """‡∏î‡∏π‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
        try:
            base_info = self.get_service_info()  # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏°
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á
            enhanced_info = {
                **base_info,
                'enhanced_features': {
                    'improved_quality_filtering': True,
                    'bbox_validation': True,
                    'intelligent_model_selection': True,
                    'performance_monitoring': True
                },
                'performance_stats': self.performance_stats,
                'quality_thresholds': {
                    'min_quality': self.decision_criteria['min_quality_threshold'],
                    'agreement_ratio': self.decision_criteria['min_agreement_ratio'],
                    'iou_threshold': self.decision_criteria['iou_threshold']
                }
            }
            
            return enhanced_info
            
        except Exception as e:
            logger.error(f"‚ùå Error getting enhanced service info: {e}")
            return {'error': str(e)}
    
    def _load_image(self, image_input: Union[str, np.ndarray]) -> np.ndarray:
        """‡πÇ‡∏´‡∏•‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö"""
        if isinstance(image_input, str):
            if not os.path.exists(image_input):
                raise FileNotFoundError(f"Image file not found: {image_input}")
            image = cv2.imread(image_input)
            if image is None:
                raise ValueError(f"Cannot read image: {image_input}")
        else:
            image = image_input
            
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        if image.shape[0] < 32 or image.shape[1] < 32:
            raise ValueError(f"Image too small: {image.shape}")
            
        return image
    
    async def _load_all_models(self) -> bool:
        """‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ error"""
        try:
            # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß (implementation ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°)
            # ... existing model loading code ...
            return True
        except Exception as e:
            logger.error(f"‚ùå Model loading failed: {e}")
            return False
### **Phase 2: ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞ Validation (Week 2)**---
# üß™ ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Face Recognition ‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô

## **‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö**

### **1. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å**
- ‚úÖ boss_04.jpg ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ confidence >95% (‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏° 51.8%)
- ‚úÖ Unknown faces ‡∏ï‡πâ‡∏≠‡∏á‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÄ‡∏õ‡πá‡∏ô "unknown"
- ‚úÖ Bounding boxes ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ (‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û)
- ‚úÖ Quality scores ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-100

### **2. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°**
- Recognition accuracy >85%
- Processing time <2 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ/‡∏†‡∏≤‡∏û
- False positive rate <10%
- Unknown detection rate ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

---

## **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö**

### **Step 1: Unit Testing - ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏¢‡πà‡∏≠‡∏¢**

#### **1.1 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Real ONNX Models**
```python
async def test_real_embedding_extraction():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏Å‡∏±‡∏î embedding ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á"""
    service = FaceRecognitionService()
    await service.initialize()
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    boss_image = cv2.imread("test_images/boss_04.jpg")
    
    # ‡∏™‡∏Å‡∏±‡∏î embedding 2 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
    embedding1 = await service.extract_embedding(boss_image)
    embedding2 = await service.extract_embedding(boss_image)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á
    similarity = service._cosine_similarity(embedding1.vector, embedding2.vector)
    
    # ‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ similarity >95%
    assert similarity > 0.95, f"Same image similarity too low: {similarity:.3f}"
    print(f"‚úÖ Same image similarity: {similarity:.3f}")
```

#### **1.2 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Quality Score Calculation**
```python
def test_quality_score_range():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"""
    # Test cases with known results
    test_cases = [
        {
            'bbox': {'x1': 100, 'y1': 100, 'x2': 300, 'y2': 300, 'confidence': 0.9},
            'image_shape': (640, 480),
            'expected_range': (70, 100)
        },
        {
            'bbox': {'x1': 0, 'y1': 0, 'x2': 640, 'y2': 480, 'confidence': 0.3},
            'image_shape': (640, 480),
            'expected_range': (0, 30)  # Full image bbox should be poor
        }
    ]
    
    for case in test_cases:
        quality = calculate_face_quality(case['bbox'], case['image_shape'])
        min_expected, max_expected = case['expected_range']
        
        assert 0 <= quality <= 100, f"Quality out of range: {quality}"
        assert min_expected <= quality <= max_expected, f"Quality {quality} not in expected range {case['expected_range']}"
        print(f"‚úÖ Quality test passed: {quality:.1f}")
```

#### **1.3 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Unknown Detection**
```python
async def test_unknown_detection():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å"""
    service = FaceRecognitionService()
    await service.initialize()
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
    known_face = cv2.imread("test_images/boss_01.jpg")
    await service.add_face_to_database("boss", known_face)
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å
    unknown_face = cv2.imread("test_images/unknown_person.jpg")
    result = await service.recognize_face(unknown_face)
    
    # ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ best_match ‡∏´‡∏£‡∏∑‡∏≠ confidence ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å
    assert result.best_match is None or result.confidence < 0.7, f"Unknown face wrongly recognized: {result.confidence}"
    print(f"‚úÖ Unknown detection works: confidence={result.confidence:.3f}")
```

### **Step 2: Integration Testing - ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏ß‡∏°**

#### **2.1 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö End-to-End Pipeline**
```python
async def test_full_pipeline():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö pipeline ‡∏Ñ‡∏£‡∏ö‡∏ß‡∏á‡∏à‡∏£ Detection + Recognition"""
    detection_service = ImprovedFaceDetectionService(vram_manager, detection_config)
    recognition_service = FaceRecognitionService(recognition_config)
    
    await detection_service.initialize()
    await recognition_service.initialize()
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á gallery
    await build_multi_embedding_gallery(recognition_service, detection_service, "test_images")
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≤‡∏á‡πÜ
    test_images = [
        ("test_images/boss_04.jpg", "boss", 0.95),  # ‡∏Ñ‡∏ß‡∏£‡πÑ‡∏î‡πâ >95%
        ("test_images/night_02.jpg", "night", 0.95),
        ("test_images/unknown_person.jpg", "unknown", 0.0)
    ]
    
    for image_path, expected_identity, min_confidence in test_images:
        image = cv2.imread(image_path)
        
        # Detection
        detection_result = await detection_service.detect_faces(image)
        assert len(detection_result.faces) > 0, f"No faces detected in {image_path}"
        
        # Recognition
        face_crop = extract_face_crop(image, detection_result.faces[0].bbox)
        recognition_result = await recognition_service.recognize_face(face_crop)
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        if expected_identity == "unknown":
            assert recognition_result.best_match is None, f"Unknown face wrongly recognized"
        else:
            assert recognition_result.best_match is not None, f"Known face not recognized"
            assert recognition_result.best_match.person_id == expected_identity, f"Wrong identity"
            assert recognition_result.confidence >= min_confidence, f"Confidence too low: {recognition_result.confidence}"
        
        print(f"‚úÖ {image_path}: {recognition_result.best_match.person_id if recognition_result.best_match else 'unknown'} ({recognition_result.confidence:.3f})")
```

#### **2.2 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Performance Benchmarks**
```python
async def test_performance_benchmarks():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û"""
    service = FaceAnalysisService(vram_manager, config)
    await service.initialize()
    
    # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_images = load_test_images("test_images")
    
    processing_times = []
    accuracy_scores = []
    
    for image_path in test_images:
        start_time = time.time()
        
        image = cv2.imread(image_path)
        result = await service.analyze_faces(image, config)
        
        processing_time = time.time() - start_time
        processing_times.append(processing_time)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ground truth)
        if has_ground_truth(image_path):
            accuracy = calculate_accuracy(result, get_ground_truth(image_path))
            accuracy_scores.append(accuracy)
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏Å‡∏ì‡∏ë‡πå
    avg_time = np.mean(processing_times)
    avg_accuracy = np.mean(accuracy_scores) if accuracy_scores else 0
    
    assert avg_time < 2.0, f"Processing too slow: {avg_time:.3f}s"
    assert avg_accuracy > 0.85, f"Accuracy too low: {avg_accuracy:.3f}"
    
    print(f"‚úÖ Performance: {avg_time:.3f}s/image, {avg_accuracy:.1%} accuracy")
```

### **Step 3: Real-world Testing - ‡∏™‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á**

#### **3.1 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏°‡πà**
```python
async def test_with_new_images():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÄ‡∏´‡πá‡∏ô"""
    # ‡∏ñ‡πà‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏°‡πà‡∏Ç‡∏≠‡∏á boss ‡πÅ‡∏•‡∏∞ night
    new_test_images = [
        "new_images/boss_new_01.jpg",
        "new_images/boss_new_02.jpg", 
        "new_images/night_new_01.jpg",
        "new_images/night_new_02.jpg",
        "new_images/stranger_01.jpg",
        "new_images/stranger_02.jpg"
    ]
    
    expected_results = {
        "boss_new": "boss",
        "night_new": "night", 
        "stranger": "unknown"
    }
    
    correct_predictions = 0
    total_predictions = 0
    
    for image_path in new_test_images:
        result = await analyze_single_image(image_path)
        
        # ‡∏î‡∏∂‡∏á expected result
        for key, expected in expected_results.items():
            if key in image_path:
                if result.identity == expected:
                    correct_predictions += 1
                total_predictions += 1
                break
    
    accuracy = correct_predictions / total_predictions
    print(f"üìä New images accuracy: {accuracy:.1%} ({correct_predictions}/{total_predictions})")
```

#### **3.2 ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Edge Cases**
```python
async def test_edge_cases():
    """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏û‡∏¥‡πÄ‡∏®‡∏©"""
    edge_cases = [
        ("very_dark_image.jpg", "low lighting"),
        ("very_bright_image.jpg", "overexposed"),
        ("blurry_image.jpg", "motion blur"),
        ("low_resolution.jpg", "low quality"),
        ("multiple_faces.jpg", "group photo"),
        ("partial_face.jpg", "face cut off"),
        ("side_profile.jpg", "non-frontal"),
        ("with_mask.jpg", "face mask"),
        ("with_sunglasses.jpg", "occlusion")
    ]
    
    results = {}
    
    for image_file, condition in edge_cases:
        try:
            result = await analyze_single_image(f"edge_cases/{image_file}")
            
            results[condition] = {
                'success': True,
                'faces_detected': len(result.faces),
                'avg_quality': np.mean([f.quality_score for f in result.faces]) if result.faces else 0,
                'processing_time': result.total_time
            }
            
        except Exception as e:
            results[condition] = {
                'success': False,
                'error': str(e)
            }
    
    # ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•
    for condition, result in results.items():
        if result['success']:
            print(f"‚úÖ {condition}: {result['faces_detected']} faces, quality {result['avg_quality']:.1f}")
        else:
            print(f"‚ùå {condition}: {result['error']}")
```

---

## **‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö**

### **‡∏£‡∏∞‡∏î‡∏±‡∏ö Critical (‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô 100%)**
- ‚úÖ boss_04.jpg confidence >95%
- ‚úÖ Quality scores ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0-100
- ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ bbox ‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û (area_ratio <90%)
- ‚úÖ Unknown faces ‡∏ñ‡∏π‡∏Å‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á >80%

### **‡∏£‡∏∞‡∏î‡∏±‡∏ö High (‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô >90%)**
- ‚úÖ Overall recognition accuracy >85%
- ‚úÖ Processing time <2s per image
- ‚úÖ False positive rate <10%
- ‚úÖ System stability (no crashes)

### **‡∏£‡∏∞‡∏î‡∏±‡∏ö Medium (‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô >70%)**
- ‚úÖ Edge cases handling >70%
- ‚úÖ Multi-face scenarios >80%
- ‚úÖ Different lighting conditions >75%

---

## **‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•**

### **Metrics ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Å‡πá‡∏ö**
```python
test_metrics = {
    'accuracy': {
        'boss_recognition': 0.0,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å boss ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        'night_recognition': 0.0, # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å night ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        'unknown_detection': 0.0,  # ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å unknown ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        'overall_accuracy': 0.0
    },
    'performance': {
        'avg_processing_time': 0.0,
        'detection_time': 0.0,
        'recognition_time': 0.0,
        'throughput_fps': 0.0
    },
    'quality': {
        'avg_detection_quality': 0.0,
        'valid_detection_ratio': 0.0,
        'bbox_accuracy': 0.0
    },
    'robustness': {
        'edge_case_success_rate': 0.0,
        'error_rate': 0.0,
        'system_stability': 0.0
    }
}
```

### **‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î**
```python
def generate_test_report(metrics, test_results):
    report = f"""
# üìã Face Recognition System Test Report

## üéØ ‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å
- boss_04.jpg confidence: {test_results['boss_04_confidence']:.1%} (‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: >95%)
- Quality score range: ‚úÖ All within 0-100
- Bbox accuracy: {metrics['quality']['bbox_accuracy']:.1%}
- Unknown detection: {metrics['accuracy']['unknown_detection']:.1%}

## üìä ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°
- Overall accuracy: {metrics['accuracy']['overall_accuracy']:.1%}
- Processing time: {metrics['performance']['avg_processing_time']:.3f}s
- Detection quality: {metrics['quality']['avg_detection_quality']:.1f}/100
- System stability: {metrics['robustness']['system_stability']:.1%}

## üîç ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô
- Boss recognition: {metrics['accuracy']['boss_recognition']:.1%}
- Night recognition: {metrics['accuracy']['night_recognition']:.1%}
- Unknown detection: {metrics['accuracy']['unknown_detection']:.1%}
- Edge cases: {metrics['robustness']['edge_case_success_rate']:.1%}

## ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
- Critical tests: {'PASS' if all_critical_passed else 'FAIL'}
- Performance tests: {'PASS' if performance_passed else 'FAIL'}
- Robustness tests: {'PASS' if robustness_passed else 'FAIL'}

Overall: {'PASS' if overall_passed else 'FAIL'}
"""
    return report
```

---

## **‡πÅ‡∏ú‡∏ô‡∏Å‡∏≤‡∏£ Deploy ‡πÅ‡∏•‡∏∞ Monitor**

### **1. Gradual Rollout**
- Week 1: Unit testing ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö
- Week 2: Integration testing ‡πÅ‡∏•‡∏∞ performance tuning  
- Week 3: Real-world testing ‡πÅ‡∏•‡∏∞ edge case handling
- Week 4: Production deployment ‡πÅ‡∏•‡∏∞ monitoring

### **2. Continuous Monitoring**
```python
# Production monitoring metrics
production_metrics = {
    'daily_accuracy': [],
    'processing_times': [],
    'error_rates': [],
    'unknown_detection_rates': [],
    'user_feedback': []
}

# Alert thresholds
alert_thresholds = {
    'accuracy_drop': 0.10,  # ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤ accuracy ‡∏•‡∏î >10%
    'slow_processing': 3.0,  # ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡∏ä‡πâ‡∏≤ >3 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
    'high_error_rate': 0.05  # ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤ error >5%
}
```

‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏î‡∏µ‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á!
## üìã **‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç**

### **üö® ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ**

| ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ | ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á | ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏ | ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö |
|-------|------------------|--------|----------|
| **‡πÉ‡∏ä‡πâ Random Embedding** | üî¥ Critical | `np.random.randn()` ‡πÅ‡∏ó‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á | boss_04 ‡πÑ‡∏î‡πâ‡πÅ‡∏Ñ‡πà 51.8% ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô >95% |
| **‡πÑ‡∏°‡πà‡∏°‡∏µ Unknown Detection** | üî¥ Critical | ‡πÑ‡∏°‡πà‡∏°‡∏µ threshold checking | ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏õ‡∏•‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡∏ú‡∏¥‡∏î 100% |
| **Quality Score ‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥** | üü† High | ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏¥‡∏î ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ >100 | ‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ú‡∏¥‡∏î |
| **Bbox ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏†‡∏≤‡∏û** | üü† High | Detection model parameters | ‡∏ï‡∏±‡∏î‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ |

### **üîß ‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ô‡∏≠**

#### **Phase 1: Core Fixes (Priority 1)**
1. **‚úÖ ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Real ONNX Models** - ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á ‡πÅ‡∏ó‡∏ô fake embedding
2. **‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏° Unknown Detection Logic** - threshold checking ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏≥‡πÅ‡∏ô‡∏Å unknown
3. **‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç Quality Calculation** - ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ 0-100 ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
4. **‚úÖ ‡∏õ‡∏£‡∏±‡∏ö Detection Parameters** - ‡πÄ‡∏û‡∏¥‡πà‡∏° confidence threshold ‡πÄ‡∏õ‡πá‡∏ô 0.4

#### **Phase 2: Enhanced Features (Priority 2)**  
1. **‚úÖ Multi-Embedding Gallery** - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ embeddings ‡∏ï‡πà‡∏≠‡∏Ñ‡∏ô
2. **‚úÖ Intelligent Model Selection** - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏°‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå
3. **‚úÖ Quality-based Filtering** - ‡∏Å‡∏£‡∏≠‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥
4. **‚úÖ Performance Monitoring** - ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û

### **üéØ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á**

#### **Before (‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô)**
```json
"boss_04.jpg": {
  "confidence": 0.518,  // ‚ùå ‡πÅ‡∏Ñ‡πà 51.8%
  "quality_score": 789,  // ‚ùå ‡πÄ‡∏Å‡∏¥‡∏ô 100
  "best_match": "boss"   // ‚ùå ‡∏ñ‡∏π‡∏Å‡πÇ‡∏î‡∏¢‡∏ö‡∏±‡∏á‡πÄ‡∏≠‡∏¥‡∏ç
}
```

#### **After (‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç)**
```json
"boss_04.jpg": {
  "confidence": 0.96,   // ‚úÖ >95%
  "quality_score": 87,  // ‚úÖ ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô 0-100
  "best_match": "boss"  // ‚úÖ ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏£‡∏¥‡∏á
}

"unknown_person.jpg": {
  "confidence": 0.0,    // ‚úÖ ‡∏à‡∏≥‡πÅ‡∏ô‡∏Å unknown ‡πÑ‡∏î‡πâ
  "best_match": null    // ‚úÖ ‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏ú‡∏¥‡∏î
}
```

### **üìà ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à**

| Metric | Current | Target | Critical |
|--------|---------|--------|----------|
| boss_04 Confidence | 51.8% | >95% | ‚úÖ |
| Unknown Detection | 0% | >80% | ‚úÖ |
| Quality Score Range | 0-789 | 0-100 | ‚úÖ |
| Processing Time | 0.67s | <2s | ‚úÖ |
| Overall Accuracy | ~76% | >85% | üéØ |

### **‚ö° ‡∏Å‡∏≤‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ**

#### **Step 1: ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà Core Service (Day 1-3)**
1. Replace `face_recognition_service.py` ‡∏î‡πâ‡∏ß‡∏¢ **Real ONNX Implementation**
2. Update `utils.py` ‡∏î‡πâ‡∏ß‡∏¢ **Fixed Quality Calculation**  
3. Test ‡∏Å‡∏±‡∏ö `boss_04.jpg` - ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ >95%

#### **Step 2: Deploy ‡πÅ‡∏•‡∏∞ Test (Day 4-7)**
1. Run `multi_embedding_face_test.py` ‡πÉ‡∏´‡∏°‡πà
2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏° **Testing Plan**
3. Fine-tune thresholds ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

#### **Step 3: Production Ready (Week 2)**
1. Complete integration testing
2. Performance optimization
3. Deploy to production

---

## üéä **‡∏™‡∏£‡∏∏‡∏õ: ‡∏à‡∏≤‡∏Å‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏π‡πà‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô**

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏•‡∏±‡∏Å:** ‡∏£‡∏∞‡∏ö‡∏ö‡πÉ‡∏ä‡πâ **Random Numbers ‡πÅ‡∏ó‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• AI ‡∏à‡∏£‡∏¥‡∏á** ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏°‡πà‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠

**‡πÇ‡∏ã‡∏•‡∏π‡∏ä‡∏±‡∏ô:** ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô **Real ONNX Models + Unknown Detection + Quality Fixes**

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á:** 
- ‚úÖ boss_04.jpg ‡∏à‡∏≤‡∏Å 51.8% ‚Üí >95%
- ‚úÖ Unknown faces ‡∏à‡∏≤‡∏Å 0% ‚Üí >80% detection rate  
- ‚úÖ System reliability ‡∏à‡∏≤‡∏Å Poor ‚Üí Excellent

**‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô:** ~1-2 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç + testing

**Return on Investment:** ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö Production ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏µ‡∏¢‡∏á Demo ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô!