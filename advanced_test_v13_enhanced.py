#!/usr/bin/env python3
"""
‡∏£‡∏∞‡∏ö‡∏ö Face Recognition ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á v13 Enhanced - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Ensemble System
- Ultra Quality Enhancement Pipeline with Super Resolution
- Advanced preprocessing ‡πÅ‡∏•‡∏∞ Multi-scale processing
- Quality-aware face cropping
- Multi-tier Threshold System
- Cross-person validation
- Context-aware recognition (single vs group photos)
- **NEW: Face Recognition Ensemble System Support**
"""

import os
import cv2
import numpy as np
import json
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, List, Optional, Any, Tuple
import asyncio
import sys

# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡πÄ‡∏û‡∏∑‡πà‡∏≠ import modules
sys.path.append('src')

from src.ai_services.face_recognition.face_recognition_service import FaceRecognitionService, RecognitionConfig
from src.ai_services.face_recognition.ensemble_face_recognition_service import EnsembleFaceRecognitionService, EnsembleConfig
from src.ai_services.face_detection.face_detection_service import FaceDetectionService
from src.ai_services.face_recognition.models import ModelType
from src.ai_services.common.vram_manager import VRAMManager

class UltraQualityEnhancer:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏û‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # Super Resolution parameters
        self.sr_scale_factor = 2  # ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ç‡∏ô‡∏≤‡∏î 2 ‡πÄ‡∏ó‡πà‡∏≤
        self.target_face_size = 224  # ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢ (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 160)
        
    def enhance_image_ultra_quality(self, image: np.ndarray) -> np.ndarray:
        """Ultra Quality Enhancement Pipeline"""
        try:
            original_height, original_width = image.shape[:2]
            self.logger.debug(f"Original size: {original_width}x{original_height}")
            
            # === STAGE 1: Noise Reduction ===
            # 1.1 Non-local means denoising (‡∏•‡∏î‡∏™‡∏±‡∏ç‡∏ç‡∏≤‡∏ì‡∏£‡∏ö‡∏Å‡∏ß‡∏ô)
            denoised = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)
            
            # === STAGE 2: Super Resolution (‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ threshold) ===
            if min(original_width, original_height) < 800:  # ‡∏ñ‡πâ‡∏≤‡∏†‡∏≤‡∏û‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 800px
                self.logger.debug("Applying super resolution...")
                denoised = self.apply_super_resolution(denoised)
            
            # === STAGE 3: Advanced Enhancement ===
            # 3.1 CLAHE with optimized parameters
            lab = cv2.cvtColor(denoised, cv2.COLOR_BGR2LAB)
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))  # ‡∏õ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå
            lab[:, :, 0] = clahe.apply(lab[:, :, 0])
            enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
            
            # 3.2 Bilateral filter (edge-preserving smoothing)
            enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
            
            # 3.3 Unsharp masking (advanced sharpening)
            enhanced = self.unsharp_mask(enhanced, sigma=1.0, strength=1.5)
            
            # 3.4 Adaptive gamma correction
            enhanced = self.adaptive_gamma_correction(enhanced)
            
            # === STAGE 4: Color Enhancement ===
            # 4.1 Saturation boost (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏¥‡πà‡∏°‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏™‡∏µ)
            hsv = cv2.cvtColor(enhanced, cv2.COLOR_BGR2HSV)
            hsv[:, :, 1] = cv2.multiply(hsv[:, :, 1], 1.2)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° saturation 20%
            enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            
            # 4.2 Contrast enhancement
            enhanced = cv2.convertScaleAbs(enhanced, alpha=1.1, beta=10)
            
            final_height, final_width = enhanced.shape[:2]
            self.logger.debug(f"Enhanced size: {final_width}x{final_height}")
            
            return enhanced
            
        except Exception as e:
            self.logger.error(f"Enhancement failed: {e}")
            return image  # ‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    
    def apply_super_resolution(self, image: np.ndarray) -> np.ndarray:
        """Apply Super Resolution using EDSR-like interpolation"""
        try:
            height, width = image.shape[:2]
            new_width = width * self.sr_scale_factor
            new_height = height * self.sr_scale_factor
            
            # ‡πÉ‡∏ä‡πâ INTER_CUBIC ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
            upscaled = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_CUBIC)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° additional sharpening
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(upscaled, -1, kernel)
            
            # ‡∏ú‡∏™‡∏°‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà sharp
            result = cv2.addWeighted(upscaled, 0.7, sharpened, 0.3, 0)
            
            return result
            
        except Exception:
            return image
    
    def unsharp_mask(self, image: np.ndarray, sigma: float = 1.0, strength: float = 1.5) -> np.ndarray:
        """Apply unsharp masking for better edge definition"""
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á blurred version
            blurred = cv2.GaussianBlur(image, (0, 0), sigma)
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á sharpened image
            sharpened = cv2.addWeighted(image, 1.0 + strength, blurred, -strength, 0)
            
            return sharpened
            
        except Exception:
            return image
    
    def adaptive_gamma_correction(self, image: np.ndarray) -> np.ndarray:
        """Apply adaptive gamma correction based on image histogram"""
        try:
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gamma value ‡∏à‡∏≤‡∏Å histogram
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            hist_normalized = hist / hist.sum()
            
            # ‡∏´‡∏≤ gamma ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            cumsum = np.cumsum(hist_normalized)
            median_idx = np.where(cumsum >= 0.5)[0][0]
            gamma = np.log(0.5) / np.log(median_idx / 255.0) if median_idx > 0 else 1.0
            gamma = np.clip(gamma, 0.5, 2.0)  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏ä‡πà‡∏ß‡∏á gamma
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á lookup table
            inv_gamma = 1.0 / gamma
            table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
            
            # Apply gamma correction
            corrected = cv2.LUT(image, table)
            
            return corrected
            
        except Exception:
            return image
    
    def assess_image_quality(self, image: np.ndarray) -> Dict[str, float]:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # 1. Sharpness (Laplacian variance)
            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
            sharpness_score = min(sharpness / 1000, 1.0)
            
            # 2. Brightness distribution
            brightness = np.mean(gray)
            brightness_score = 1.0 - abs(brightness - 128) / 128
            
            # 3. Contrast (standard deviation)
            contrast = np.std(gray)
            contrast_score = min(contrast / 64, 1.0)
            
            # 4. Noise level (high frequency content)
            noise_level = np.std(cv2.Laplacian(gray, cv2.CV_64F))
            noise_score = 1.0 - min(noise_level / 100, 1.0)
            
            # 5. Overall quality
            overall_quality = (sharpness_score * 0.4 + brightness_score * 0.2 + 
                             contrast_score * 0.2 + noise_score * 0.2)
            
            return {
                'sharpness': sharpness_score,
                'brightness': brightness_score,
                'contrast': contrast_score,
                'noise': noise_score,
                'overall': overall_quality
            }
            
        except Exception:
            return {'overall': 0.5}
    
    def crop_face_ultra_quality(self, image: np.ndarray, bbox, target_size: int = 224) -> np.ndarray:
        """‡∏ï‡∏±‡∏î‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î"""
        try:
            height, width = image.shape[:2]
            
            # ‡∏Ç‡∏¢‡∏≤‡∏¢‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            margin = 0.3  # ‡πÄ‡∏û‡∏¥‡πà‡∏° margin ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô
            x1 = max(0, int(bbox.x1 - bbox.width * margin))
            y1 = max(0, int(bbox.y1 - bbox.height * margin))
            x2 = min(width, int(bbox.x2 + bbox.width * margin))
            y2 = min(height, int(bbox.y2 + bbox.height * margin))
            
            # ‡∏ï‡∏±‡∏î‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            face_crop = image[y1:y2, x1:x2]
            
            if face_crop.size == 0:
                # Fallback ‡∏ñ‡πâ‡∏≤‡∏Å‡∏≤‡∏£‡∏Ç‡∏¢‡∏≤‡∏¢‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤
                x1, y1 = max(0, int(bbox.x1)), max(0, int(bbox.y1))
                x2, y2 = min(width, int(bbox.x2)), min(height, int(bbox.y2))
                face_crop = image[y1:y2, x1:x2]
            
            # Resize to target size with high quality interpolation
            if face_crop.size > 0:
                face_crop = cv2.resize(face_crop, (target_size, target_size), 
                                     interpolation=cv2.INTER_LANCZOS4)
                
                # Apply additional enhancement to face crop
                face_crop = self.enhance_face_crop(face_crop)
            
            return face_crop
            
        except Exception as e:
            self.logger.error(f"‚ùå Error cropping face: {e}")
            return np.array([])
    
    def enhance_face_crop(self, face_crop: np.ndarray) -> np.ndarray:
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡πÅ‡∏•‡πâ‡∏ß"""
        try:
            # 1. Histogram equalization
            lab = cv2.cvtColor(face_crop, cv2.COLOR_BGR2LAB)
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))
            lab[:, :, 0] = clahe.apply(lab[:, :, 0])
            enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
            
            # 2. Slight sharpening
            kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])
            enhanced = cv2.filter2D(enhanced, -1, kernel)
            
            # 3. Noise reduction
            enhanced = cv2.bilateralFilter(enhanced, 5, 50, 50)
            
            return enhanced
            
        except Exception:
            return face_crop


class AdvancedRealImageTestSystemV13Enhanced:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö Face Recognition ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏° Ensemble Support"""
    
    def __init__(self, use_ensemble: bool = True):
        self.setup_logging()
        self.output_dir = Path("output/advanced_real_image_test_v13_enhanced")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # === NEW: Ultra Quality Enhancer ===
        self.ultra_enhancer = UltraQualityEnhancer()
        self.logger.info("üöÄ Ultra Quality Enhancement initialized")
        
        # === NEW: Ensemble Support ===
        self.use_ensemble = use_ensemble
        
        if self.use_ensemble:
            self.logger.info("üéØ Ensemble mode enabled - using AdaFace + FaceNet + ArcFace")
            self.ensemble_config = EnsembleConfig(
                adaface_weight=0.25,   # 25% ‡∏ï‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
                facenet_weight=0.50,   # 50% ‡∏ï‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
                arcface_weight=0.25,   # 25% ‡∏ï‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
                ensemble_threshold=0.20,
                enable_gpu_optimization=True,
                quality_threshold=0.2
            )
        else:
            self.logger.info("üîß Individual model mode - using FaceNet")
            # === ADVANCED: Multi-tier Threshold System ===
            self.config = RecognitionConfig(
                similarity_threshold=0.60,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.55 ‡πÄ‡∏õ‡πá‡∏ô 0.60 (‡∏Å‡∏•‡∏≤‡∏á‡πÜ)
                unknown_threshold=0.55,     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.50 ‡πÄ‡∏õ‡πá‡∏ô 0.55
                quality_threshold=0.2,
                preferred_model=ModelType.FACENET
            )
        
        # === NEW: Multi-tier Thresholds ===
        self.thresholds = {
            'high_confidence': 0.85,    # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏™‡∏π‡∏á
            'medium_confidence': 0.70,  # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏õ‡∏≤‡∏ô‡∏Å‡∏•‡∏≤‡∏á  
            'low_confidence': 0.55,     # ‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ï‡πà‡∏≥ (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô)
            'unknown_boundary': 0.50    # ‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ö‡πà‡∏á unknown
        }
        
        # === VRAM Management ===
        vram_config = {
            "total_vram_mb": 12288,  # RTX 3060 12GB
            "reserved_system_mb": 2048,
            "model_vram_estimates": {
                "yolov9c-face": 512 * 1024 * 1024,
                "yolov9e-face": 2048 * 1024 * 1024,
                "yolov11m-face": 2 * 1024 * 1024 * 1024,
                "adaface": 89 * 1024 * 1024,
                "arcface": 249 * 1024 * 1024,
                "facenet": 249 * 1024 * 1024,
            }
        }
        self.vram_manager = VRAMManager(vram_config)
        
        # === Services ===
        if self.use_ensemble:
            self.face_service = EnsembleFaceRecognitionService(self.ensemble_config, self.vram_manager)
        else:
            self.face_service = FaceRecognitionService(self.config, self.vram_manager)
        
        # ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ detection ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
        detection_config = {
            "yolov9c_model_path": "model/face-detection/yolov9c-face-lindevs.onnx",
            "yolov9e_model_path": "model/face-detection/yolov9e-face-lindevs.onnx",
            "yolov11m_model_path": "model/face-detection/yolov11m-face.pt",
            "max_usable_faces_yolov9": 8,
            "min_agreement_ratio": 0.7,
            "min_quality_threshold": 60,
            "conf_threshold": 0.15,
            "iou_threshold": 0.4,
            "img_size": 640
        }
        self.detection_service = FaceDetectionService(self.vram_manager, detection_config)
        
        # ‡πÄ‡∏Å‡πá‡∏ö embeddings ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
        self.registered_people = {}
        self.dynamic_embeddings = {}
        
        # === NEW: Disable dynamic embeddings ===
        self.enable_dynamic_embeddings = False  # ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≠
        
        # === NEW: Reference Image Tracking ===
        self.reference_images = set()
        
        # === NEW: Context tracking ===
        self.recognition_stats = {
            'total_faces': 0,
            'false_positives_prevented': 0,
            'cross_person_rejections': 0,
            'group_photo_rejections': 0,
            'high_confidence_matches': 0
        }
        
        self._initialized = False
        
    def setup_logging(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('advanced_real_image_test_v13_enhanced.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    async def initialize_services(self):
        """Initialize all services"""
        if not self._initialized:
            await self.face_service.initialize()
            await self.detection_service.initialize()
            self._initialized = True
            self.logger.info("‚úÖ Services initialized successfully")

    async def enroll_person(self, image_path: str, person_name: str) -> bool:
        """‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏ô"""
        try:
            if not os.path.exists(image_path):
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {image_path}")
                return False
                
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠ reference images
            filename = os.path.basename(image_path)
            self.reference_images.add(filename)
                
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏†‡∏≤‡∏û
            image = cv2.imread(image_path)
            if image is None:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ: {image_path}")
                return False
                
            # === ULTRA QUALITY ENHANCEMENT ===
            enhanced_image = self.ultra_enhancer.enhance_image_ultra_quality(image)
            
            # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            quality_metrics = self.ultra_enhancer.assess_image_quality(enhanced_image)
            self.logger.debug(f"Image quality: {quality_metrics}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            detection_result = await self.detection_service.detect_faces(enhanced_image)
            
            if not detection_result.faces:
                self.logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô: {image_path}")
                return False
                
            # ‡πÉ‡∏ä‡πâ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            best_face = max(detection_result.faces, key=lambda f: f.bbox.confidence)
            
            # === ULTRA QUALITY FACE CROPPING ===
            face_crop = self.ultra_enhancer.crop_face_ultra_quality(
                enhanced_image, best_face.bbox, target_size=224
            )
            
            if face_crop.size == 0:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏î‡πâ: {image_path}")
                return False
            
            if self.use_ensemble:
                # ‡πÉ‡∏ä‡πâ Ensemble service
                success = await self.face_service.add_face_to_database(
                    person_name, face_crop,
                    metadata={
                        'source_image': image_path,
                        'detection_confidence': best_face.bbox.confidence,
                        'quality_metrics': quality_metrics,
                        'enhancement_applied': True
                    }
                )
            else:
                # ‡πÉ‡∏ä‡πâ Individual service
                embedding = await self.face_service.extract_embedding(face_crop)
                if embedding is None:
                    self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡πÑ‡∏î‡πâ: {image_path}")
                    return False
                    
                # ‡πÄ‡∏Å‡πá‡∏ö embedding
                if person_name not in self.registered_people:
                    self.registered_people[person_name] = []
                    
                self.registered_people[person_name].append({
                    'embedding': embedding.vector,
                    'source_image': image_path,
                    'quality': best_face.bbox.confidence,
                    'bbox': best_face.bbox,
                    'enrollment_time': datetime.now().isoformat(),
                    'is_reference': True
                })
                success = True
            
            if success:
                self.logger.info(f"‚úÖ ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {person_name} ‡∏à‡∏≤‡∏Å {image_path} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (Quality: {best_face.bbox.confidence:.3f})")
            
            return success
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {image_path}: {e}")
            return False

    async def enroll_reference_images(self) -> bool:
        """‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        mode_text = "Ensemble System" if self.use_ensemble else "Individual Model"
        self.logger.info(f"üìù ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ {mode_text}...")
        
        # ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á - boss_01-10 ‡πÅ‡∏•‡∏∞ night_01-10
        reference_files = []
        
        # Boss images (boss_01 ‡∏ñ‡∏∂‡∏á boss_10)
        for i in range(1, 11):
            reference_files.append((f"test_images/boss_{i:02d}.jpg", "Boss"))
        
        # Night images (night_01 ‡∏ñ‡∏∂‡∏á night_10)  
        for i in range(1, 11):
            reference_files.append((f"test_images/night_{i:02d}.jpg", "Night"))
        
        total_registered = 0
        
        for image_path, person_name in reference_files:
            if await self.enroll_person(image_path, person_name):
                total_registered += 1
                
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
        self.logger.info("=" * 50)
        self.logger.info(f"üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {mode_text}:")
        
        if self.use_ensemble:
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ensemble
            ensemble_stats = self.face_service.get_statistics()
            for person_name in ['Boss', 'Night']:
                person_data = self.face_service.face_database.get(person_name, [])
                self.logger.info(f"   üë§ {person_name}: {len(person_data)} ‡∏†‡∏≤‡∏û")
            
            self.logger.info(f"   üìà ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_registered} ‡∏†‡∏≤‡∏û")
            self.logger.info(f"   üéØ Ensemble weights: AdaFace 25%, FaceNet 50%, ArcFace 25%")
            self.logger.info(f"   üîß Model success rates:")
            for model, stats in ensemble_stats['model_success_rates'].items():
                rate = stats.get('success_rate', 0) * 100 if stats['total'] > 0 else 0
                self.logger.info(f"      - {model.upper()}: {rate:.1f}%")
        else:
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ individual
            for person_name, embeddings in self.registered_people.items():
                self.logger.info(f"   üë§ {person_name}: {len(embeddings)} ‡∏†‡∏≤‡∏û")
            self.logger.info(f"   üìà ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_registered} ‡∏†‡∏≤‡∏û")
            self.logger.info(f"   üîß Model: FaceNet VGGFace2")
        
        self.logger.info(f"   üéØ Face crop size: 224x224 (Ultra Quality)")
        
        return total_registered > 0

    async def recognize_face_in_image(self, image_path: str) -> List[Dict[str, Any]]:
        """‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ö"""
        try:
            if not os.path.exists(image_path):
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {image_path}")
                return []
                
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏†‡∏≤‡∏û
            image = cv2.imread(image_path)
            if image is None:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ: {image_path}")
                return []
                
            # === ULTRA QUALITY ENHANCEMENT ===
            enhanced_image = self.ultra_enhancer.enhance_image_ultra_quality(image)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            detection_result = await self.detection_service.detect_faces(enhanced_image)
            
            if not detection_result.faces:
                self.logger.info(f"‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô: {os.path.basename(image_path)}")
                return []
            
            # ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            face_results = []
            
            for i, detected_face in enumerate(detection_result.faces):
                try:
                    # === ULTRA QUALITY FACE CROPPING ===
                    face_crop = self.ultra_enhancer.crop_face_ultra_quality(
                        enhanced_image, detected_face.bbox, target_size=224
                    )
                    
                    if face_crop.size == 0:
                        continue
                    
                    # ‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
                    if self.use_ensemble:
                        recognition_result = await self.face_service.recognize_face(face_crop)
                    else:
                        embedding = await self.face_service.extract_embedding(face_crop)
                        if embedding is None:
                            continue
                        recognition_result = await self.individual_recognize_face(embedding)
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                    face_result = {
                        'face_id': i,
                        'bbox': {
                            'x1': detected_face.bbox.x1,
                            'y1': detected_face.bbox.y1,
                            'x2': detected_face.bbox.x2,
                            'y2': detected_face.bbox.y2,
                            'confidence': detected_face.bbox.confidence
                        },
                        'recognition': {
                            'identity': recognition_result.best_match.identity_id if recognition_result.best_match else "Unknown",
                            'confidence': recognition_result.confidence,
                            'similarity': recognition_result.best_match.similarity if recognition_result.best_match else 0.0,
                            'is_known': recognition_result.is_known,
                            'processing_time': recognition_result.processing_time
                        },
                        'quality_metrics': self.ultra_enhancer.assess_image_quality(face_crop)
                    }
                    
                    face_results.append(face_result)
                    
                except Exception as e:
                    self.logger.error(f"‚ùå Error processing face {i}: {e}")
                    continue
            
            self.logger.info(f"üîç ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• {len(face_results)} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô {os.path.basename(image_path)}")
            
            return face_results
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ {image_path}: {e}")
            return []

    async def individual_recognize_face(self, embedding) -> Any:
        """‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö individual model (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ ensemble)"""
        try:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á mock result object ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô‡πÑ‡∏î‡πâ
            class MockMatch:
                def __init__(self, identity_id, similarity):
                    self.identity_id = identity_id
                    self.similarity = similarity
            
            class MockResult:
                def __init__(self):
                    self.best_match = None
                    self.confidence = 0.0
                    self.is_known = False
                    self.processing_time = 0.0
            
            result = MockResult()
            
            # ‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢
            best_similarity = 0.0
            best_person = None
            
            for person_name, person_embeddings in self.registered_people.items():
                for emb_data in person_embeddings:
                    similarity = np.dot(embedding.vector, emb_data['embedding'])
                    if similarity > best_similarity:
                        best_similarity = similarity
                        best_person = person_name
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö threshold
            threshold = self.config.similarity_threshold
            if best_similarity >= threshold:
                result.best_match = MockMatch(best_person, best_similarity)
                result.confidence = best_similarity / threshold
                result.is_known = True
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå Individual recognition failed: {e}")
            class MockResult:
                def __init__(self):
                    self.best_match = None
                    self.confidence = 0.0
                    self.is_known = False
                    self.processing_time = 0.0
            return MockResult()

    async def run_comprehensive_test(self):
        """‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡∏ö‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô"""
        try:
            mode_text = "Ensemble System" if self.use_ensemble else "Individual Model (FaceNet)"
            self.logger.info(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö {mode_text}")
            self.logger.info("=" * 80)
            
            # Initialize services
            await self.initialize_services()
            
            # Enroll reference images
            enrollment_success = await self.enroll_reference_images()
            
            if not enrollment_success:
                self.logger.error("‚ùå ‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß")
                return
            
            # Test images
            test_files = [
                "test_images/boss_01.jpg",
                "test_images/boss_05.jpg", 
                "test_images/boss_10.jpg",
                "test_images/night_01.jpg",
                "test_images/night_05.jpg",
                "test_images/night_10.jpg",
                "test_images/boss_11.jpg",  # Unknown
                "test_images/boss_glass02.jpg",  # Modified
            ]
            
            all_results = []
            
            for test_file in test_files:
                if os.path.exists(test_file):
                    self.logger.info(f"üß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {os.path.basename(test_file)}")
                    results = await self.recognize_face_in_image(test_file)
                    all_results.extend(results)
                    
                    for result in results:
                        identity = result['recognition']['identity']
                        confidence = result['recognition']['confidence']
                        self.logger.info(f"   üë§ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {identity} (confidence: {confidence:.3f})")
                else:
                    self.logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {test_file}")
            
            # Save results
            self.save_test_results(all_results)
            
            # Show summary
            self.show_test_summary(all_results)
            
            self.logger.info("=" * 80)
            self.logger.info(f"‚úÖ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö {mode_text} ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!")
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
            raise

    def save_test_results(self, results: List[Dict[str, Any]]):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            mode_suffix = "ensemble" if self.use_ensemble else "individual"
            results_file = self.output_dir / f"test_results_{mode_suffix}_{timestamp}.json"
            
            output_data = {
                'test_info': {
                    'timestamp': datetime.now().isoformat(),
                    'mode': 'ensemble' if self.use_ensemble else 'individual',
                    'model_config': self.ensemble_config.__dict__ if self.use_ensemble else self.config.__dict__,
                    'total_faces_tested': len(results)
                },
                'results': results
            }
            
            if self.use_ensemble:
                output_data['ensemble_statistics'] = self.face_service.get_statistics()
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"üíæ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà: {results_file}")
            
        except Exception as e:
            self.logger.error(f"‚ùå Error saving results: {e}")

    def show_test_summary(self, results: List[Dict[str, Any]]):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
        try:
            if not results:
                self.logger.info("‚ÑπÔ∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á")
                return
            
            # ‡∏ô‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            identity_counts = {}
            confidence_scores = []
            known_faces = 0
            
            for result in results:
                identity = result['recognition']['identity']
                confidence = result['recognition']['confidence']
                is_known = result['recognition']['is_known']
                
                identity_counts[identity] = identity_counts.get(identity, 0) + 1
                confidence_scores.append(confidence)
                
                if is_known:
                    known_faces += 1
            
            self.logger.info("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
            self.logger.info(f"   üî¢ ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {len(results)}")
            self.logger.info(f"   ‚úÖ ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å: {known_faces}")
            self.logger.info(f"   ‚ùì ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å: {len(results) - known_faces}")
            
            if confidence_scores:
                avg_confidence = np.mean(confidence_scores)
                self.logger.info(f"   üìà ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_confidence:.3f}")
            
            self.logger.info("   üë• ‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡∏ï‡∏≤‡∏°‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•:")
            for identity, count in identity_counts.items():
                percentage = (count / len(results)) * 100
                self.logger.info(f"      - {identity}: {count} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ({percentage:.1f}%)")
            
            if self.use_ensemble:
                # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ ensemble
                ensemble_stats = self.face_service.get_statistics()
                self.logger.info("   üîß ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥ Ensemble:")
                self.logger.info(f"      - Total extractions: {ensemble_stats['total_extractions']}")
                self.logger.info(f"      - Total recognitions: {ensemble_stats['total_recognitions']}")
                if ensemble_stats['ensemble_processing_times']:
                    avg_time = np.mean(ensemble_stats['ensemble_processing_times'])
                    self.logger.info(f"      - Average processing time: {avg_time:.3f}s")
                
        except Exception as e:
            self.logger.error(f"‚ùå Error showing summary: {e}")


async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    try:
        print("üéØ Face Recognition Test System V13 Enhanced")
        print("=" * 50)
        print("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏´‡∏°‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
        print("1. Ensemble System (AdaFace + FaceNet + ArcFace)")
        print("2. Individual Model (FaceNet)")
        print("3. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö")
        
        choice = input("‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÄ‡∏•‡∏∑‡∏≠‡∏Å (1/2/3): ").strip()
        
        if choice == "1":
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Ensemble
            test_system = AdvancedRealImageTestSystemV13Enhanced(use_ensemble=True)
            await test_system.run_comprehensive_test()
            
        elif choice == "2":
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Individual
            test_system = AdvancedRealImageTestSystemV13Enhanced(use_ensemble=False)
            await test_system.run_comprehensive_test()
            
        elif choice == "3":
            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡πÅ‡∏ö‡∏ö
            print("\nüîß ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Individual Model ‡∏Å‡πà‡∏≠‡∏ô...")
            test_system1 = AdvancedRealImageTestSystemV13Enhanced(use_ensemble=False)
            await test_system1.run_comprehensive_test()
            
            print("\nüéØ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Ensemble System...")
            test_system2 = AdvancedRealImageTestSystemV13Enhanced(use_ensemble=True)
            await test_system2.run_comprehensive_test()
            
        else:
            print("‚ùå ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á")
        
    except Exception as e:
        logging.error(f"‚ùå ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
