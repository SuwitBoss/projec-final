#!/usr/bin/env python3
"""
Enhanced Face Recognition Test System - FIXED VERSION
‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÅ‡∏•‡πâ‡∏ß
- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç False Negative ‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°
- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç False Positive (Night ‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Night)
- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á threshold ‡πÅ‡∏•‡∏∞ model weights
- ‡πÄ‡∏û‡∏¥‡πà‡∏° context-aware recognition
"""

import cv2
import numpy as np
import json
import time
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from enum import Enum

# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡πÄ‡∏û‡∏∑‡πà‡∏≠ import modules
import sys
sys.path.append('src')

from src.ai_services.face_recognition.face_recognition_service import FaceRecognitionService, RecognitionConfig
from src.ai_services.face_detection.face_detection_service import FaceDetectionService
from src.ai_services.face_recognition.models import ModelType
from src.ai_services.common.vram_manager import VRAMManager

class ImageType(Enum):
    """‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
    REFERENCE = "reference"
    GROUP = "group"
    GLASSES = "glasses"
    FACE_SWAP = "face_swap"
    SPOOFING = "spoofing"
    REGISTERED = "registered"

class EnhancedGraFIQsQualityAssessment:
    """Enhanced Training-free Face Image Quality Assessment - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def assess_quality(self, face_image: np.ndarray) -> float:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• - Enhanced Version"""
        try:
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            if len(face_image.shape) == 3:
                gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
            else:
                gray = face_image
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient magnitudes
            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            mean_grad = np.mean(gradient_magnitude)
            std_grad = np.std(gradient_magnitude)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì edge density
            edge_threshold = mean_grad + std_grad * 0.5  # ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î
            edge_pixels = np.sum(gradient_magnitude > edge_threshold)
            edge_density = edge_pixels / gradient_magnitude.size
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì sharpness score (Laplacian)
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì brightness ‡πÅ‡∏•‡∏∞ contrast
            brightness = np.mean(gray)
            contrast = np.std(gray)
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            quality_score = (
                0.25 * min(mean_grad / 40.0, 1.0) +      # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 50.0
                0.20 * min(edge_density * 8, 1.0) +      # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 10
                0.25 * min(laplacian_var / 800.0, 1.0) + # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 1000.0
                0.15 * min(std_grad / 25.0, 1.0) +       # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 30.0
                0.10 * min(contrast / 40.0, 1.0) +       # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤ contrast
                0.05 * (1.0 - abs(brightness - 127.5) / 127.5)  # brightness balance
            )
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô 0-100 ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° baseline
            base_quality = max(0.0, quality_score * 100 + 5.0)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° baseline 5 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô
            return float(np.clip(base_quality, 0, 100))
            
        except Exception as e:
            self.logger.error(f"Quality assessment failed: {e}")
            return 50.0  # Default medium quality

class EnhancedUltraQualityEnhancer:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏û‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á - Enhanced Version"""
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.target_face_size = 224
        self.use_lab_colorspace = False  # ‡∏õ‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ LAB colorspace ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏™‡∏µ‡πÄ‡∏û‡∏µ‡πâ‡∏¢‡∏ô
    
    def _preserve_original_colors(self, original: np.ndarray, processed: np.ndarray) -> np.ndarray:
        """‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏µ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ú‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡πà‡∏≤‡∏¢"""
        try:
            # ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡∏á‡πà‡∏≤‡∏¢‡πÜ ‡πÇ‡∏î‡∏¢‡∏ú‡∏™‡∏°‡∏†‡∏≤‡∏û‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏±‡∏ö‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏µ‡∏ò‡∏£‡∏£‡∏°‡∏ä‡∏≤‡∏ï‡∏¥ (90% processed + 10% original)
            corrected = cv2.addWeighted(processed, 0.9, original, 0.1, 0)
            
            self.logger.debug("Simple color preservation applied (90% processed + 10% original)")
            return corrected
            
        except Exception as e:
            self.logger.error(f"Color preservation failed: {e}")
            return processed

    def _final_color_correction(self, original: np.ndarray, processed: np.ndarray) -> np.ndarray:
        """‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏™‡∏µ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢ histogram matching"""
        try:
            # ‡πÉ‡∏ä‡πâ histogram matching ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏µ‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
            corrected = processed.copy()
            
            for channel in range(3):
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì histogram ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ channel
                orig_hist = cv2.calcHist([original], [channel], None, [256], [0, 256])
                proc_hist = cv2.calcHist([processed], [channel], None, [256], [0, 256])
                
                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì CDF
                orig_cdf = orig_hist.cumsum()
                proc_cdf = proc_hist.cumsum()
                
                # Normalize
                orig_cdf = orig_cdf / (orig_cdf[-1] + 1e-7)
                proc_cdf = proc_cdf / (proc_cdf[-1] + 1e-7)
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á lookup table
                lut = np.zeros(256, dtype=np.uint8)
                for i in range(256):
                    # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
                    diff = np.abs(proc_cdf - orig_cdf[i])
                    lut[i] = np.argmin(diff)
                
                # Apply lookup table ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡πà‡∏≠‡∏ô
                corrected[:, :, channel] = cv2.LUT(processed[:, :, channel], lut)
            
            # ‡∏ú‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏™‡∏µ (15% ‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö)
            final = cv2.addWeighted(corrected, 0.85, original, 0.15, 0)
            
            return final
            
        except Exception as e:
            self.logger.error(f"Final color correction failed: {e}")
            return processed
    def enhance_image_ultra_quality(self, image: np.ndarray) -> np.ndarray:
        """Ultra Quality Enhancement - Fixed Color Issues (No LAB Processing)"""
        try:
            original_height, original_width = image.shape[:2]
            self.logger.debug(f"Original size: {original_width}x{original_height}")
            
            # ‡πÄ‡∏Å‡πá‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö color reference
            image_for_color_ref = image.copy()
            enhanced = image.copy()
            
            # === STAGE 1: Simple noise reduction ===
            enhanced = cv2.fastNlMeansDenoisingColored(enhanced, None, 3, 3, 7, 21)
            
            # === STAGE 2: Very light contrast adjustment ===
            enhanced = cv2.convertScaleAbs(enhanced, alpha=1.05, beta=5)
            
            # === STAGE 3: Light bilateral filter ===
            enhanced = cv2.bilateralFilter(enhanced, 3, 30, 30)
            
            # === STAGE 4: Preserve original colors strongly ===
            enhanced = cv2.addWeighted(enhanced, 0.7, image_for_color_ref, 0.3, 0)
            
            return enhanced
            
        except Exception as e:
            self.logger.error(f"Enhanced processing failed: {e}")
            return image

    def enhance_group_photo(self, image: np.ndarray) -> np.ndarray:
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°"""
        try:
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà (‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤)
            height, width = image.shape[:2]
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° contrast ‡πÅ‡∏•‡∏∞ brightness ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            enhanced = cv2.convertScaleAbs(image, alpha=1.2, beta=15)
            
            # ‡∏•‡∏î noise ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡∏≤
            enhanced = cv2.bilateralFilter(enhanced, 5, 40, 40)
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° sharpness ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
            kernel = np.array([[-0.5, -0.5, -0.5],
                              [-0.5,  5.0, -0.5],
                              [-0.5, -0.5, -0.5]])
            sharpened = cv2.filter2D(enhanced, -1, kernel)
            enhanced = cv2.addWeighted(enhanced, 0.8, sharpened, 0.2, 0)
            
            return enhanced
            
        except Exception:
            return image
    
    def adaptive_gamma_correction_v3(self, image: np.ndarray) -> np.ndarray:
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á gamma correction ‡πÅ‡∏ö‡∏ö adaptive - Version 3"""
        try:
            lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
            l_channel = lab[:, :, 0]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì histogram
            hist = cv2.calcHist([l_channel], [0], None, [256], [0, 256])
            hist_norm = hist.ravel() / hist.sum()
            cdf = hist_norm.cumsum()
            
            # ‡∏´‡∏≤ gamma ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            median_val = np.where(cdf >= 0.5)[0][0]
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç gamma
            if median_val < 90:  # ‡∏†‡∏≤‡∏û‡∏°‡∏∑‡∏î (‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 85)
                gamma = 0.7  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.6
            elif median_val > 160:  # ‡∏†‡∏≤‡∏û‡∏™‡∏ß‡πà‡∏≤‡∏á (‡∏•‡∏î‡∏à‡∏≤‡∏Å 170)
                gamma = 1.3  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 1.4
            else:
                gamma = 1.0
                
            # Apply gamma correction
            inv_gamma = 1.0 / gamma
            table = np.array([((i / 255.0) ** inv_gamma) * 255 
                            for i in np.arange(0, 256)]).astype("uint8")
            corrected = cv2.LUT(image, table)
            
            return corrected
            
        except Exception:
            return image
    
    def crop_face_ultra_quality_v3(self, image: np.ndarray, bbox, target_size: int = None) -> np.ndarray:
        """Enhanced face cropping with improved sizing strategy - Version 3"""
        try:
            if target_size is None:
                target_size = self.target_face_size
                
            height, width = image.shape[:2]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì face center
            face_center_x = (bbox.x1 + bbox.x2) / 2
            face_center_y = (bbox.y1 + bbox.y2) / 2
            face_width = bbox.x2 - bbox.x1
            face_height = bbox.y2 - bbox.y1
            
            # ‡∏õ‡∏£‡∏±‡∏ö margin ‡πÅ‡∏ö‡∏ö adaptive - Enhanced
            face_size = min(face_width, face_height)
            if face_size < 64:
                margin_factor = 0.6  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.5
            elif face_size < 128:
                margin_factor = 0.5  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.4
            else:
                margin_factor = 0.4  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.3
                
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î crop ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            crop_size = max(face_width, face_height) * (1 + margin_factor)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì crop coordinates
            x1 = max(0, int(face_center_x - crop_size / 2))
            y1 = max(0, int(face_center_y - crop_size / 2))
            x2 = min(width, int(face_center_x + crop_size / 2))
            y2 = min(height, int(face_center_y + crop_size / 2))
            
            # Crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            face_crop = image[y1:y2, x1:x2]
            
            if face_crop.size == 0:
                return np.array([])
            
            # Enhanced preprocessing ‡∏Å‡πà‡∏≠‡∏ô resize
            if face_crop.shape[0] < 112 or face_crop.shape[1] < 112:
                # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏•‡πá‡∏Å ‡πÉ‡∏ä‡πâ super-resolution technique ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
                scale_factor = max(2, 112 // min(face_crop.shape[:2]))
                temp_size = (face_crop.shape[1] * scale_factor, face_crop.shape[0] * scale_factor)
                face_crop = cv2.resize(face_crop, temp_size, interpolation=cv2.INTER_CUBIC)
            
            # ‡πÉ‡∏ä‡πâ INTER_LANCZOS4 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            if face_crop.shape[0] != target_size or face_crop.shape[1] != target_size:
                face_crop = cv2.resize(face_crop, (target_size, target_size), 
                                     interpolation=cv2.INTER_LANCZOS4)
            
            return face_crop
            
        except Exception as e:
            self.logger.error(f"Ultra quality cropping v3 failed: {e}")
            return np.array([])

@dataclass
class TestResult:
    """‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏û"""
    image_path: str
    image_type: ImageType
    faces_detected: int
    faces_recognized: int
    recognition_results: List[Dict[str, Any]]
    processing_time: float
    quality_metrics: Dict[str, float]
    grafiqs_quality: float
    success: bool
    error_message: Optional[str] = None

class EnhancedFaceRecognitionTest:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á - Enhanced Version"""
    
    def __init__(self, test_images_dir: str = "D:/projec-final/test_images"):
        self.setup_logging()
        self.test_images_dir = Path(test_images_dir)
        self.output_dir = Path("output/enhanced_test_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # === Enhanced Quality Components ===
        self.ultra_enhancer = EnhancedUltraQualityEnhancer()
        self.grafiqs_quality = EnhancedGraFIQsQualityAssessment()
        self.logger.info("üöÄ Enhanced Ultra Quality Enhancement initialized")
        
        # === Enhanced Configuration ===
        self.config = RecognitionConfig(
            similarity_threshold=0.50,  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.60
            unknown_threshold=0.45,     # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.55
            quality_threshold=0.15,     # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.2
            preferred_model=None
        )
        
        # === Enhanced Model Weights ===
        self.model_weights = {
            'facenet': 0.40,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.30
            'adaface': 0.35,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 0.40
            'arcface': 0.25   # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.30
        }
        
        # === Enhanced Multi-tier Thresholds ===
        self.thresholds = {
            'high_confidence': 0.70,    # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.75
            'medium_confidence': 0.55,  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.65
            'low_confidence': 0.45,     # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.55
            'rejection': 0.40,          # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.50
            'cross_person_gap': 0.12,   # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.15
            'group_photo_penalty': 0.00,
            'grafiqs_quality_threshold': 30.0,  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 40.0
            
            # ‡πÄ‡∏û‡∏¥‡πà‡∏° context-specific thresholds
            'group_photo_threshold': 0.40,      # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°
            'registered_photo_threshold': 0.60, # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
            'quality_bonus_threshold': 70.0     # ‡πÉ‡∏´‡πâ‡πÇ‡∏ö‡∏ô‡∏±‡∏™‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
        }
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô VRAM manager
        vram_config = {
            "reserved_vram_mb": 512,
            "model_vram_estimates": {
                "yolov9c-face": 512 * 1024 * 1024,
                "yolov9e-face": 2048 * 1024 * 1024,
                "yolov11m-face": 2 * 1024 * 1024 * 1024,
                "adaface": 89 * 1024 * 1024,
                "arcface": 249 * 1024 * 1024,
                "facenet": 249 * 1024 * 1024,
            }
        }
        self.vram_manager = VRAMManager(vram_config)
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô services
        self.face_service = FaceRecognitionService(self.config, self.vram_manager)
        
        detection_config = {
            "yolov9c_model_path": "model/face-detection/yolov9c-face-lindevs.onnx",
            "yolov9e_model_path": "model/face-detection/yolov9e-face-lindevs.onnx",
            "yolov11m_model_path": "model/face-detection/yolov11m-face.pt",
            "max_usable_faces_yolov9": 10,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 8
            "min_agreement_ratio": 0.6,     # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.7
            "min_quality_threshold": 50,    # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 60
            "conf_threshold": 0.12,         # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.15
            "iou_threshold": 0.4,
            "img_size": 640
        }
        self.detection_service = FaceDetectionService(self.vram_manager, detection_config)
        
        # ‡πÄ‡∏Å‡πá‡∏ö embeddings ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
        self.registered_people = {}
        
        # === Enhanced Test Statistics ===
        self.test_stats = {
            'total_tests': 0,
            'successful_tests': 0,
            'total_faces_detected': 0,
            'total_faces_recognized': 0,
            'recognition_by_type': {},
            'quality_distribution': {},
            'processing_times': [],
            'error_count': 0,
            'context_aware_matches': 0,
            'quality_bonus_applied': 0
        }
        
        self._initialized = False
    
    def convert_numpy_types(self, obj):
        """‡πÅ‡∏õ‡∏•‡∏á numpy types ‡πÄ‡∏õ‡πá‡∏ô Python types ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ serialization ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: self.convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_numpy_types(item) for item in obj]
        else:
            return obj
    
    def setup_logging(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('enhanced_face_test.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    async def initialize_services(self):
        """Initialize all services"""
        if not self._initialized:
            await self.face_service.initialize()
            await self.detection_service.initialize()
            self._initialized = True
            self.logger.info("‚úÖ Enhanced services initialized successfully")

    def get_test_images(self) -> Dict[ImageType, List[str]]:
        """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"""
        reference_files = [
            ("boss_01.jpg", "Boss"), ("boss_02.jpg", "Boss"), ("boss_03.jpg", "Boss"),
            ("boss_04.jpg", "Boss"), ("boss_05.jpg", "Boss"), ("boss_06.jpg", "Boss"),
            ("boss_07.jpg", "Boss"), ("boss_08.jpg", "Boss"), ("boss_09.jpg", "Boss"),
            ("boss_10.jpg", "Boss"), ("night_01.jpg", "Night"), ("night_02.jpg", "Night"),
            ("night_03.jpg", "Night"), ("night_04.jpg", "Night"), ("night_05.jpg", "Night"),
            ("night_06.jpg", "Night"), ("night_07.jpg", "Night"), ("night_08.jpg", "Night"),
            ("night_09.jpg", "Night"), ("night_10.jpg", "Night")
        ]
        
        test_images = {
            ImageType.REFERENCE: [filename for filename, _ in reference_files],
            
            ImageType.GROUP: [
                "night_group01.jpg", "night_group02.jpg", 
                "boss_group01.jpg", "boss_group02.jpg", "boss_group03.jpg"
            ],
            
            ImageType.GLASSES: [
                "boss_glass02.jpg", "boss_glass03.jpg", "boss_glass04.jpg"
            ],
            
            ImageType.FACE_SWAP: [
                "face-swap01.png", "face-swap02.png", "face-swap03.png"
            ],
            
            ImageType.SPOOFING: [
                "spoofing_01.jpg", "spoofing_02.jpg"
            ],
            
            ImageType.REGISTERED: [
                "boss_01.jpg", "boss_05.jpg", "boss_10.jpg",
                "night_01.jpg", "night_05.jpg", "night_10.jpg"
            ]
        }
        
        return test_images

    async def enroll_person_enhanced(self, image_path: str, person_name: str) -> bool:
        """‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î - Enhanced Version"""
        try:
            full_path = self.test_images_dir / image_path
            if not full_path.exists():
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {full_path}")
                return False
                
            image = cv2.imread(str(full_path))
            if image is None:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ: {full_path}")
                return False
                
            # === ENHANCED ULTRA QUALITY ENHANCEMENT ===
            enhanced_image = self.ultra_enhancer.enhance_image_ultra_quality(image)
            
            # === Enhanced GraFIQs Quality Assessment ===
            grafiqs_score = self.grafiqs_quality.assess_quality(enhanced_image)
            
            if grafiqs_score < self.thresholds['grafiqs_quality_threshold']:
                self.logger.warning(f"‚ö†Ô∏è Low quality image rejected: {image_path} (GraFIQs: {grafiqs_score:.1f})")
                return False
                
            self.logger.info(f"üìä Enhanced GraFIQs Quality Score: {grafiqs_score:.1f}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            detection_result = await self.detection_service.detect_faces(enhanced_image)
            if not detection_result.faces:
                self.logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô: {image_path}")
                return False
            
            best_face = max(detection_result.faces, key=lambda f: f.bbox.confidence)
            
            # === ENHANCED ULTRA QUALITY FACE CROPPING ===
            face_crop = self.ultra_enhancer.crop_face_ultra_quality_v3(
                enhanced_image, best_face.bbox, target_size=224
            )
            if face_crop.size == 0:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏î‡πâ: {image_path}")
                return False
            
            # ‡∏î‡∏∂‡∏á embeddings ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            model_embeddings = await self.extract_embeddings_from_all_models(face_crop)
            
            if not model_embeddings:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÑ‡∏î‡πâ: {image_path}")
                return False
                
            # ‡πÄ‡∏Å‡πá‡∏ö embedding ‡∏û‡∏£‡πâ‡∏≠‡∏° Enhanced metadata
            if person_name not in self.registered_people:
                self.registered_people[person_name] = []
                
            self.registered_people[person_name].append({
                'model_embeddings': model_embeddings,
                'source_image': str(full_path),
                'quality': best_face.bbox.confidence,
                'grafiqs_quality': grafiqs_score,
                'bbox': best_face.bbox,
                'enrollment_time': datetime.now().isoformat(),
                'is_reference': True,
                'enhanced_processing': True
            })
            
            model_count = len(model_embeddings)
            self.logger.info(f"‚úÖ Enhanced enrollment {person_name} ‡∏à‡∏≤‡∏Å {image_path} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à "
                           f"(Quality: {best_face.bbox.confidence:.3f}, GraFIQs: {grafiqs_score:.1f}, "
                           f"Models: {model_count}/3)")
            return True
                 
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {image_path}: {e}")
            return False

    async def enroll_reference_images(self) -> bool:
        """‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î - Enhanced Version"""
        self.logger.info("üìù ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢ Enhanced Processing...")
        
        reference_files = [
            ("boss_01.jpg", "Boss"), ("boss_02.jpg", "Boss"), ("boss_03.jpg", "Boss"),
            ("boss_04.jpg", "Boss"), ("boss_05.jpg", "Boss"), ("boss_06.jpg", "Boss"),
            ("boss_07.jpg", "Boss"), ("boss_08.jpg", "Boss"), ("boss_09.jpg", "Boss"),
            ("boss_10.jpg", "Boss"), ("night_01.jpg", "Night"), ("night_02.jpg", "Night"),
            ("night_03.jpg", "Night"), ("night_04.jpg", "Night"), ("night_05.jpg", "Night"),
            ("night_06.jpg", "Night"), ("night_07.jpg", "Night"), ("night_08.jpg", "Night"),
            ("night_09.jpg", "Night"), ("night_10.jpg", "Night")
        ]
        
        total_registered = 0
        
        for image_path, person_name in reference_files:
            if await self.enroll_person_enhanced(image_path, person_name):
                total_registered += 1
                
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ Enhanced
        self.logger.info("=" * 60)
        self.logger.info("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Enhanced Processing:")
        for person_name, embeddings in self.registered_people.items():
            avg_quality = np.mean([emb['quality'] for emb in embeddings])
            avg_grafiqs = np.mean([emb['grafiqs_quality'] for emb in embeddings])
            self.logger.info(f"   üë§ {person_name}: {len(embeddings)} ‡∏†‡∏≤‡∏û "
                           f"(Avg Quality: {avg_quality:.3f}, Avg GraFIQs: {avg_grafiqs:.1f})")
        self.logger.info(f"   üìà ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_registered} ‡∏†‡∏≤‡∏û")
        self.logger.info("   üéØ Enhanced Processing: ‚úÖ")
        self.logger.info("   üî¨ Using Enhanced LAB + GraFIQs + Context-Aware Recognition")
        
        return total_registered > 0

    async def enhanced_context_aware_matching(self, model_embeddings: Dict[str, np.ndarray], 
                                            image_type: ImageType, 
                                            face_count: int,
                                            grafiqs_quality: float) -> Optional[Dict[str, Any]]:
        """Enhanced Context-Aware Face Matching"""
        if not model_embeddings:
            return None
            
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì similarity ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model_similarities = {}
        
        for model_name, target_embedding in model_embeddings.items():
            person_similarities = {}
            
            for person_name, embeddings_data in self.registered_people.items():
                max_similarities = []
                
                for embedding_data in embeddings_data:
                    if 'model_embeddings' in embedding_data and model_name in embedding_data['model_embeddings']:
                        ref_embedding = embedding_data['model_embeddings'][model_name]
                        similarity = np.dot(target_embedding, ref_embedding) / (
                            np.linalg.norm(target_embedding) * np.linalg.norm(ref_embedding) + 1e-7
                        )
                        max_similarities.append(similarity)
                
                if max_similarities:
                    person_similarities[person_name] = max(max_similarities)
            
            if person_similarities:
                model_similarities[model_name] = person_similarities
        
        # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ Enhanced weighted ensemble
        ensemble_similarities = {}
        
        for person_name in self.registered_people.keys():
            weighted_sum = 0.0
            total_weight = 0.0
            
            for model_name, weight in self.model_weights.items():
                if model_name in model_similarities and person_name in model_similarities[model_name]:
                    weighted_sum += model_similarities[model_name][person_name] * weight
                    total_weight += weight
            
            if total_weight > 0:
                ensemble_similarities[person_name] = weighted_sum / total_weight
        
        if not ensemble_similarities:
            return None
        
        # ‡∏´‡∏≤‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ similarity ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        best_person = max(ensemble_similarities.keys(), 
                         key=lambda x: ensemble_similarities[x])
        best_similarity = ensemble_similarities[best_person]
        
        # ‡∏´‡∏≤‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ similarity ‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 2
        second_best_similarity = 0.0
        if len(ensemble_similarities) > 1:
            similarities_list = [(person, similarity) 
                               for person, similarity in ensemble_similarities.items()]
            similarities_list.sort(key=lambda x: x[1], reverse=True)
            if len(similarities_list) > 1:
                second_best_similarity = similarities_list[1][1]
        
        # === ENHANCED CONTEXT-AWARE THRESHOLD SELECTION ===
        similarity_gap = best_similarity - second_best_similarity
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡πÅ‡∏ö‡∏ö context-aware
        if image_type == ImageType.REGISTERED:
            base_threshold = self.thresholds['registered_photo_threshold']
        elif image_type == ImageType.GROUP:
            base_threshold = self.thresholds['group_photo_threshold']
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° face count
            if face_count > 10:
                base_threshold *= 0.85  # ‡∏•‡∏î threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÉ‡∏´‡∏ç‡πà
            elif face_count > 5:
                base_threshold *= 0.90
        elif image_type == ImageType.FACE_SWAP:
            base_threshold = self.thresholds['high_confidence']
        elif image_type == ImageType.SPOOFING:
            base_threshold = self.thresholds['high_confidence']
        else:
            base_threshold = self.thresholds['medium_confidence']
        
        # Quality bonus - ‡πÉ‡∏´‡πâ‡πÇ‡∏ö‡∏ô‡∏±‡∏™‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
        quality_factor = 1.0
        if grafiqs_quality > self.thresholds['quality_bonus_threshold']:
            quality_factor = 0.95  # ‡∏•‡∏î threshold 5% ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏î‡∏µ
            self.test_stats['quality_bonus_applied'] += 1
        
        final_threshold = base_threshold * quality_factor
        
        # Enhanced validation
        cross_person_threshold = self.thresholds['cross_person_gap']
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç Enhanced
        if best_similarity < final_threshold:
            return None
            
        if similarity_gap < cross_person_threshold:
            # ‡∏û‡∏¥‡πÄ‡∏®‡∏©: ‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÅ‡∏•‡∏∞‡∏°‡∏µ similarity ‡∏™‡∏π‡∏á
            if image_type == ImageType.GROUP and best_similarity > 0.55:
                self.logger.info(f"üéØ Context-aware allowance for group photo: {best_similarity:.3f}")
                self.test_stats['context_aware_matches'] += 1
            else:
                return None
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        model_results = {}
        for model_name in model_similarities:
            if best_person in model_similarities[model_name]:
                model_results[model_name] = float(model_similarities[model_name][best_person])
        
        result = {
            'person_name': best_person,
            'confidence': float(best_similarity),
            'similarity_gap': float(similarity_gap),
            'threshold_used': float(final_threshold),
            'base_threshold': float(base_threshold),
            'quality_factor': float(quality_factor),
            'image_type': image_type.value,
            'context_aware': True,
            'model_results': model_results
        }
        
        return self.convert_numpy_types(result)

    async def test_single_image_enhanced(self, image_path: str, image_type: ImageType) -> TestResult:
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ö - Enhanced Version"""
        start_time = time.time()
        
        try:
            full_path = self.test_images_dir / image_path
            
            if not full_path.exists():
                return TestResult(
                    image_path=image_path,
                    image_type=image_type,
                    faces_detected=0,
                    faces_recognized=0,
                    recognition_results=[],
                    processing_time=0,
                    quality_metrics={},
                    grafiqs_quality=0,
                    success=False,
                    error_message=f"File not found: {full_path}"
                )
            
            image = cv2.imread(str(full_path))
            if image is None:
                return TestResult(
                    image_path=image_path,
                    image_type=image_type,
                    faces_detected=0,
                    faces_recognized=0,
                    recognition_results=[],
                    processing_time=0,
                    quality_metrics={},
                    grafiqs_quality=0,
                    success=False,
                    error_message="Cannot read image"
                )
            
            # === ENHANCED ULTRA QUALITY ENHANCEMENT ===
            enhanced_image = self.ultra_enhancer.enhance_image_ultra_quality(image)
            
            # === Enhanced GraFIQs Quality Assessment ===
            grafiqs_quality = self.grafiqs_quality.assess_quality(enhanced_image)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            detection_result = await self.detection_service.detect_faces(enhanced_image)
            
            recognition_results = []
            faces_recognized = 0
            face_count = len(detection_result.faces) if detection_result.faces else 0
            
            if detection_result.faces:
                for i, face in enumerate(detection_result.faces):
                    # === ENHANCED ULTRA QUALITY FACE CROPPING ===
                    face_crop = self.ultra_enhancer.crop_face_ultra_quality_v3(
                        enhanced_image, face.bbox, target_size=224
                    )
                    if face_crop.size > 0:
                        # ‡∏î‡∏∂‡∏á embeddings ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                        model_embeddings = await self.extract_embeddings_from_all_models(face_crop)
                        
                        if model_embeddings:
                            # ‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢ Enhanced Context-Aware Matching
                            best_match = await self.enhanced_context_aware_matching(
                                model_embeddings, image_type, face_count, grafiqs_quality
                            )
                            
                            result = {
                                'face_index': i,
                                'bbox': {
                                    'x': int(face.bbox.x1),
                                    'y': int(face.bbox.y1),
                                    'width': int(face.bbox.x2 - face.bbox.x1),
                                    'height': int(face.bbox.y2 - face.bbox.y1)
                                },
                                'detection_confidence': face.bbox.confidence,
                                'person_name': best_match['person_name'] if best_match else 'unknown',
                                'recognition_confidence': best_match['confidence'] if best_match else 0.0,
                                'grafiqs_quality': grafiqs_quality,
                                'enhanced_processing': True,
                                'context_aware': best_match.get('context_aware', False) if best_match else False,
                                'model_results': best_match['model_results'] if best_match else {}
                            }
                            
                            if best_match:
                                faces_recognized += 1
                                
                            recognition_results.append(result)
            
            processing_time = time.time() - start_time
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            await self.save_result_image_enhanced(enhanced_image, recognition_results, image_path, image_type)
            
            return TestResult(
                image_path=image_path,
                image_type=image_type,
                faces_detected=face_count,
                faces_recognized=faces_recognized,
                recognition_results=recognition_results,
                processing_time=processing_time,
                quality_metrics={},
                grafiqs_quality=grafiqs_quality,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö {image_path}: {e}")
            
            return TestResult(
                image_path=image_path,
                image_type=image_type,
                faces_detected=0,
                faces_recognized=0,
                recognition_results=[],
                processing_time=processing_time,
                quality_metrics={},
                grafiqs_quality=0,
                success=False,
                error_message=str(e)
            )

    async def save_result_image_enhanced(self, image: np.ndarray, recognition_results: List[Dict], 
                                       image_path: str, image_type: ImageType):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå - Enhanced Version"""
        try:
            result_image = image.copy()
            
            for result in recognition_results:
                bbox = result['bbox']
                person_name = result['person_name']
                confidence = result['recognition_confidence']
                context_aware = result.get('context_aware', False)
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Enhanced
                if person_name == 'unknown':
                    color = (0, 0, 255)  # ‡πÅ‡∏î‡∏á
                    label = "UNKNOWN"
                elif person_name in ['Boss', 'Night']:
                    if confidence > 0.65:
                        color = (0, 200, 0) if not context_aware else (0, 255, 100)  # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡πÄ‡∏Ç‡πâ‡∏°/‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏≠‡πà‡∏≠‡∏ô
                    else:
                        color = (0, 255, 0) if not context_aware else (0, 255, 150)  # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏õ‡∏Å‡∏ï‡∏¥/‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏≠‡πà‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤
                    label = f"{person_name.upper()}"
                    if context_aware:
                        label += "*"  # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢ * ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö context-aware
                else:
                    color = (255, 0, 0)  # ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô
                    label = f"{person_name.upper()}"
                
                # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö
                cv2.rectangle(
                    result_image,
                    (bbox['x'], bbox['y']),
                    (bbox['x'] + bbox['width'], bbox['y'] + bbox['height']),
                    color, 3
                )
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                if person_name != 'unknown':
                    text = f"{label} ({confidence:.1%})"
                else:
                    text = label
                    
                # ‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                font_scale = 0.6
                thickness = 2
                (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
                
                # ‡∏ß‡∏≤‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                cv2.rectangle(
                    result_image,
                    (bbox['x'], bbox['y'] - text_height - 10),
                    (bbox['x'] + text_width, bbox['y']),
                    color, -1
                )
                
                # ‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                cv2.putText(
                    result_image,
                    text,
                    (bbox['x'], bbox['y'] - 5),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    font_scale,
                    (255, 255, 255),
                    thickness
                )
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
            output_filename = f"enhanced_result_{image_type.value}_{Path(image_path).stem}.jpg"
            output_path = self.output_dir / output_filename
            cv2.imwrite(str(output_path), result_image)
            
        except Exception as e:
            self.logger.error(f"‚ùå Error saving enhanced result image: {e}")

    async def run_enhanced_comprehensive_test(self):
        """‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° - Enhanced Version"""
        try:
            self.logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° Enhanced Face Recognition System")
            self.logger.info("=" * 80)
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: Initialize services
            await self.initialize_services()
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
            if not await self.enroll_reference_images():
                self.logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏î‡πâ")
                return
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö
            test_images = self.get_test_images()
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏†‡∏≤‡∏û
            all_results = []
            total_start_time = time.time()
            
            for image_type, image_list in test_images.items():
                if image_type == ImageType.REFERENCE:
                    continue
                    
                self.logger.info(f"\nüß™ Enhanced Testing {image_type.value.upper()} Images...")
                self.logger.info("-" * 60)
                
                type_results = []
                for image_path in image_list:
                    self.logger.info(f"üîç Enhanced Testing: {image_path}")
                    result = await self.test_single_image_enhanced(image_path, image_type)
                    type_results.append(result)
                    all_results.append(result)
                    
                    # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                    self.test_stats['total_tests'] += 1
                    if result.success:
                        self.test_stats['successful_tests'] += 1
                        self.test_stats['total_faces_detected'] += result.faces_detected
                        self.test_stats['total_faces_recognized'] += result.faces_recognized
                        self.test_stats['processing_times'].append(result.processing_time)
                    else:
                        self.test_stats['error_count'] += 1
                    
                    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                    if image_type.value not in self.test_stats['recognition_by_type']:
                        self.test_stats['recognition_by_type'][image_type.value] = {
                            'total': 0, 'detected': 0, 'recognized': 0
                        }
                    
                    self.test_stats['recognition_by_type'][image_type.value]['total'] += 1
                    self.test_stats['recognition_by_type'][image_type.value]['detected'] += result.faces_detected
                    self.test_stats['recognition_by_type'][image_type.value]['recognized'] += result.faces_recognized
                
                # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏µ‡πâ
                self.summarize_type_results_enhanced(image_type, type_results)
            
            total_time = time.time() - total_start_time
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• Enhanced
            report_path = await self.generate_enhanced_comprehensive_report(all_results, total_time)
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Enhanced
            self.display_enhanced_final_summary(total_time, report_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á: {e}")
            import traceback
            traceback.print_exc()

    def summarize_type_results_enhanced(self, image_type: ImageType, results: List[TestResult]):
        """‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó - Enhanced Version"""
        if not results:
            return
            
        total_images = len(results)
        successful_tests = len([r for r in results if r.success])
        total_faces_detected = sum(r.faces_detected for r in results)
        total_faces_recognized = sum(r.faces_recognized for r in results)
        avg_processing_time = np.mean([r.processing_time for r in results if r.success])
        avg_grafiqs_quality = np.mean([r.grafiqs_quality for r in results if r.success])
        
        recognition_rate = (total_faces_recognized / total_faces_detected * 100) if total_faces_detected > 0 else 0
        
        # ‡∏ô‡∏±‡∏ö context-aware matches
        context_aware_matches = 0
        for result in results:
            for recognition_result in result.recognition_results:
                if recognition_result.get('context_aware', False):
                    context_aware_matches += 1
        
        self.logger.info(f"üìä Enhanced ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• {image_type.value.upper()}:")
        self.logger.info(f"   üìÅ ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_images}")
        self.logger.info(f"   ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {successful_tests}")
        self.logger.info(f"   üë• ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: {total_faces_detected} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üéØ ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ: {total_faces_recognized} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üìà ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥: {recognition_rate:.1f}%")
        self.logger.info(f"   üß† Context-Aware Matches: {context_aware_matches}")
        self.logger.info(f"   ‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_processing_time:.3f}s")
        self.logger.info(f"   üî¨ Enhanced GraFIQs ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_grafiqs_quality:.1f}")

    async def generate_enhanced_comprehensive_report(self, results: List[TestResult], total_time: float) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° - Enhanced Version"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏° Enhanced
        total_tests = len(results)
        successful_tests = len([r for r in results if r.success])
        total_faces_detected = sum(r.faces_detected for r in results)
        total_faces_recognized = sum(r.faces_recognized for r in results)
        overall_recognition_rate = (total_faces_recognized / total_faces_detected * 100) if total_faces_detected > 0 else 0
        
        processing_times = [r.processing_time for r in results if r.success]
        avg_processing_time = np.mean(processing_times) if processing_times else 0
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô JSON Enhanced
        report_data = {
            'test_metadata': {
                'timestamp': datetime.now().isoformat(),
                'test_duration': total_time,
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'error_count': total_tests - successful_tests,
                'success_rate': (successful_tests / total_tests * 100) if total_tests > 0 else 0,
                'enhanced_processing': True
            },
            'recognition_performance': {
                'total_faces_detected': total_faces_detected,
                'total_faces_recognized': total_faces_recognized,
                'overall_recognition_rate': overall_recognition_rate,
                'average_processing_time': avg_processing_time,
                'fps': 1.0 / avg_processing_time if avg_processing_time > 0 else 0
            },
            'enhanced_features': {
                'context_aware_matches': self.test_stats['context_aware_matches'],
                'quality_bonus_applied': self.test_stats['quality_bonus_applied'],
                'enhanced_model_weights': self.model_weights,
                'enhanced_thresholds': self.thresholds
            },
            'recognition_by_type': self.test_stats['recognition_by_type'],
            'quality_analysis': {
                'avg_grafiqs_scores': {},
                'quality_distribution': {}
            },
            'system_improvements': {
                'enhanced_lab_colorspace': True,
                'enhanced_grafiqs_assessment': True,
                'enhanced_face_cropping_v3': True,
                'context_aware_recognition': True,
                'target_face_size': 224,
                'quality_threshold': self.thresholds['grafiqs_quality_threshold']
            },
            'detailed_results': []
        }
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Enhanced
        for image_type in ImageType:
            if image_type == ImageType.REFERENCE:
                continue
                
            type_results = [r for r in results if r.image_type == image_type]
            if type_results:
                grafiqs_scores = [r.grafiqs_quality for r in type_results if r.success]
                if grafiqs_scores:
                    report_data['quality_analysis']['avg_grafiqs_scores'][image_type.value] = np.mean(grafiqs_scores)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏û
        for result in results:
            result_dict = {
                'image_path': result.image_path,
                'image_type': result.image_type.value,
                'faces_detected': result.faces_detected,
                'faces_recognized': result.faces_recognized,
                'processing_time': result.processing_time,
                'grafiqs_quality': result.grafiqs_quality,
                'success': result.success,
                'enhanced_processing': True,
                'recognition_results': result.recognition_results
            }
            
            if result.error_message:
                result_dict['error_message'] = result.error_message
                
            report_data['detailed_results'].append(result_dict)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô JSON Enhanced
        json_filename = f"enhanced_comprehensive_test_report_{timestamp}.json"
        json_path = self.output_dir / json_filename
        
        report_data_converted = self.convert_numpy_types(report_data)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data_converted, f, ensure_ascii=False, indent=2)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Markdown Enhanced
        md_filename = f"enhanced_comprehensive_test_report_{timestamp}.md"
        md_path = self.output_dir / md_filename
        
        md_content = f"""# ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Enhanced Face Recognition ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°

## üöÄ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Enhanced

**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {total_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ  
**‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** Enhanced LAB Color Space + Enhanced GraFIQs + Context-Aware Recognition

## üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Enhanced

### ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏° Enhanced
- **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {total_tests} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:** {successful_tests} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ({successful_tests/total_tests*100:.1f}%)
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö:** {total_faces_detected} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ:** {total_faces_recognized} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- **‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡∏£‡∏ß‡∏°:** {overall_recognition_rate:.1f}%
- **Context-Aware Matches:** {self.test_stats['context_aware_matches']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- **Quality Bonus Applied:** {self.test_stats['quality_bonus_applied']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- **‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢:** {avg_processing_time:.3f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
- **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:** {1.0/avg_processing_time if avg_processing_time > 0 else 0:.1f} FPS

### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö Enhanced
‚úÖ **Enhanced LAB Color Space** - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏™‡∏á‡πÅ‡∏•‡∏∞‡∏™‡∏µ  
‚úÖ **Enhanced GraFIQs Assessment** - ‡∏•‡∏î threshold ‡πÄ‡∏õ‡πá‡∏ô {self.thresholds['grafiqs_quality_threshold']}  
‚úÖ **Enhanced Face Cropping V3** - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á margin ‡πÅ‡∏•‡∏∞ super-resolution  
‚úÖ **Context-Aware Recognition** - ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏° context  
‚úÖ **Enhanced Model Weights** - FaceNet {self.model_weights['facenet']:.0%}, AdaFace {self.model_weights['adaface']:.0%}, ArcFace {self.model_weights['arcface']:.0%}  
‚úÖ **Quality Bonus System** - ‡∏•‡∏î threshold ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á

## üìà ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏†‡∏≤‡∏û Enhanced

"""
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Enhanced
        for image_type, stats in report_data['recognition_by_type'].items():
            if stats['total'] > 0:
                recognition_rate = (stats['recognized'] / stats['detected'] * 100) if stats['detected'] > 0 else 0
                detection_rate = (stats['detected'] / stats['total'])
                
                md_content += f"""### {image_type.upper()} Images Enhanced
- **‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {stats['total']} ‡∏†‡∏≤‡∏û
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö:** {stats['detected']} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {detection_rate:.1f} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤/‡∏†‡∏≤‡∏û)
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ:** {stats['recognized']} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- **‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥:** {recognition_rate:.1f}%

"""
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û Enhanced
        md_content += """## üî¨ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û Enhanced (GraFIQs)

"""
        
        for image_type, avg_score in report_data['quality_analysis']['avg_grafiqs_scores'].items():
            md_content += f"- **{image_type.upper()}:** {avg_score:.1f}/100\n"
        
        md_content += f"""

## üéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏° Enhanced

‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô:

1. **Enhanced LAB Color Space Processing** - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏™‡∏µ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏á
2. **Context-Aware Recognition** - ‡∏õ‡∏£‡∏±‡∏ö threshold ‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
3. **Enhanced Model Weights** - ‡∏õ‡∏£‡∏±‡∏ö‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
4. **Quality Bonus System** - ‡πÉ‡∏´‡πâ‡πÇ‡∏ö‡∏ô‡∏±‡∏™‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á
5. **Enhanced Face Cropping V3** - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á margin ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° super-resolution
6. **Lowered Thresholds** - ‡∏•‡∏î threshold ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏ß
7. **Enhanced GraFIQs** - ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û

## üìä Enhanced Features Performance

- **Context-Aware Matches:** {self.test_stats['context_aware_matches']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- **Quality Bonus Applied:** {self.test_stats['quality_bonus_applied']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- **Enhanced Threshold Reductions:**
  - Similarity: 0.60 ‚Üí 0.50
  - Unknown: 0.55 ‚Üí 0.45
  - GraFIQs Quality: 40.0 ‚Üí 30.0
  - Group Photo: Special 0.40

## üí° ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠ Enhanced

### ‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô (1-2 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)
- Fine-tune context-aware thresholds ‡∏ï‡πà‡∏≠‡πÑ‡∏õ
- ‡πÄ‡∏û‡∏¥‡πà‡∏° ensemble diversity scoring

### ‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏•‡∏≤‡∏á (1-2 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)  
- ‡πÄ‡∏û‡∏¥‡πà‡∏° attention mechanism ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏∏‡πà‡∏°
- ‡∏û‡∏±‡∏í‡∏ô‡∏≤ adaptive preprocessing

### ‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß (3-6 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)
- ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Vision Transformers
- ‡∏û‡∏±‡∏í‡∏ô‡∏≤ dynamic threshold learning

---
*‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ Enhanced Face Recognition Test System v3.0*  
*‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ Enhanced Context-Aware Recognition ‡∏õ‡∏µ 2025*
"""
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        self.logger.info(f"üìÑ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° Enhanced: {json_path}")
        self.logger.info(f"üìÑ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Enhanced Markdown: {md_path}")
        
        return str(md_path)

    def display_enhanced_final_summary(self, total_time: float, report_path: str):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ - Enhanced Version"""
        self.logger.info("=" * 80)
        self.logger.info("üéâ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° Enhanced Face Recognition System")
        self.logger.info("=" * 80)
        
        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô Enhanced
        total_tests = self.test_stats['total_tests']
        successful_tests = self.test_stats['successful_tests']
        total_faces_detected = self.test_stats['total_faces_detected']
        total_faces_recognized = self.test_stats['total_faces_recognized']
        
        overall_recognition_rate = (total_faces_recognized / total_faces_detected * 100) if total_faces_detected > 0 else 0
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        avg_processing_time = np.mean(self.test_stats['processing_times']) if self.test_stats['processing_times'] else 0
        
        self.logger.info("üìä **Enhanced ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏°:**")
        self.logger.info(f"   üß™ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_tests} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        self.logger.info(f"   ‚úÖ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {success_rate:.1f}% ({successful_tests}/{total_tests})")
        self.logger.info(f"   üë• ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: {total_faces_detected} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üéØ ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ: {total_faces_recognized} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üìà ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡∏£‡∏ß‡∏°: {overall_recognition_rate:.1f}%")
        self.logger.info(f"   üß† Context-Aware Matches: {self.test_stats['context_aware_matches']}")
        self.logger.info(f"   üéÅ Quality Bonus Applied: {self.test_stats['quality_bonus_applied']} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        self.logger.info(f"   ‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_processing_time:.3f}s")
        self.logger.info(f"   üïê ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        
        self.logger.info("\nüî¨ **Enhanced ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:**")
        self.logger.info("   ‚úÖ Enhanced LAB Color Space Processing")
        self.logger.info(f"   ‚úÖ Enhanced GraFIQs Assessment (Threshold: {self.thresholds['grafiqs_quality_threshold']})")
        self.logger.info("   ‚úÖ Enhanced Face Cropping V3 (224x224 + Super-resolution)")
        self.logger.info("   ‚úÖ Context-Aware Recognition System")
        self.logger.info("   ‚úÖ Enhanced Model Weights (FaceNet 40%, AdaFace 35%, ArcFace 25%)")
        self.logger.info("   ‚úÖ Quality Bonus System")
        self.logger.info("   ‚úÖ Adaptive Threshold System")
        
        self.logger.info("\nüìà **Enhanced ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:**")
        for image_type, stats in self.test_stats['recognition_by_type'].items():
            if stats['total'] > 0:
                recognition_rate = (stats['recognized'] / stats['detected'] * 100) if stats['detected'] > 0 else 0
                self.logger.info(f"   üìÅ {image_type.upper()}: {recognition_rate:.1f}% "
                               f"({stats['recognized']}/{stats['detected']} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ ‡∏à‡∏≤‡∏Å {stats['total']} ‡∏†‡∏≤‡∏û)")
        
        # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏° Enhanced
        baseline_rate = 13.1  # ‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
        if overall_recognition_rate > baseline_rate:
            improvement = overall_recognition_rate - baseline_rate
            self.logger.info("\nüéâ **Enhanced ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!**")
            self.logger.info(f"   üìä ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: +{improvement:.1f}% (‡∏à‡∏≤‡∏Å {baseline_rate}% ‡πÄ‡∏õ‡πá‡∏ô {overall_recognition_rate:.1f}%)")
            self.logger.info(f"   üöÄ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå: {improvement/baseline_rate*100:.1f}%")
            self.logger.info(f"   üéØ Context-Aware ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°: {self.test_stats['context_aware_matches']} cases")
        else:
            self.logger.info("\n‚ö†Ô∏è **‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:**")
            self.logger.info(f"   üìä ‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {overall_recognition_rate:.1f}% vs baseline {baseline_rate}%")
        
        self.logger.info(f"\nüìÑ **Enhanced ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°:** {report_path}")
        self.logger.info(f"üóÇÔ∏è **Enhanced ‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:** {self.output_dir}")
        self.logger.info("\nüöÄ **Enhanced ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**")
        self.logger.info("üí° Enhanced System ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô")
        
    async def extract_embeddings_from_all_models(self, face_image: np.ndarray) -> Dict[str, Any]:
        """‡∏î‡∏∂‡∏á embeddings ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö (FaceNet, AdaFace, ArcFace)"""
        embeddings = {}
        models = [ModelType.FACENET, ModelType.ADAFACE, ModelType.ARCFACE]
        model_names = ['facenet', 'adaface', 'arcface']

        for i, model_type in enumerate(models):
            try:
                embedding = await self.face_service.extract_embedding(face_image)
                if embedding:
                    embeddings[model_names[i]] = embedding.vector
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á embedding ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• {model_names[i]}: {e}")

        return self.convert_numpy_types(embeddings)


async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å Enhanced"""
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö Enhanced
        test_system = EnhancedFaceRecognitionTest()
        
        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏° Enhanced
        await test_system.run_enhanced_comprehensive_test()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö Enhanced ‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô Enhanced System: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())