#!/usr/bin/env python3
"""
Comprehensive Face Recognition Test System
‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ï‡∏≤‡∏°‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢ 2023-2025
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏†‡∏≤‡∏û
- ‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
"""

import os
import cv2
import numpy as np
import json
import time
import asyncio
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum

# ‡πÄ‡∏û‡∏¥‡πà‡∏° path ‡πÄ‡∏û‡∏∑‡πà‡∏≠ import modules
import sys
sys.path.append('src')

from src.ai_services.face_recognition.face_recognition_service import FaceRecognitionService, RecognitionConfig
from src.ai_services.face_detection.face_detection_service import FaceDetectionService
from src.ai_services.face_recognition.models import ModelType
from src.ai_services.common.vram_manager import VRAMManager

class ImageType(Enum):
    """‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö"""
    REFERENCE = "reference"
    GROUP = "group"
    GLASSES = "glasses"
    FACE_SWAP = "face_swap"
    SPOOFING = "spoofing"
    REGISTERED = "registered"

class GraFIQsQualityAssessment:
    """Training-free Face Image Quality Assessment using Gradient Magnitudes"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
    def assess_quality(self, face_image: np.ndarray) -> float:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
        try:
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô grayscale ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
            if len(face_image.shape) == 3:
                gray = cv2.cvtColor(face_image, cv2.COLOR_BGR2GRAY)
            else:
                gray = face_image
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient magnitudes
            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
            mean_grad = np.mean(gradient_magnitude)
            std_grad = np.std(gradient_magnitude)
            max_grad = np.max(gradient_magnitude)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì edge density
            edge_threshold = mean_grad + std_grad
            edge_pixels = np.sum(gradient_magnitude > edge_threshold)
            edge_density = edge_pixels / gradient_magnitude.size
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì sharpness score
            laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # ‡∏£‡∏ß‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
            quality_score = (
                0.3 * min(mean_grad / 50.0, 1.0) +     # Gradient strength
                0.2 * min(edge_density * 10, 1.0) +    # Edge density  
                0.3 * min(laplacian_var / 1000.0, 1.0) + # Sharpness
                0.2 * min(std_grad / 30.0, 1.0)        # Gradient consistency
            )
            
            # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô 0-100
            return float(np.clip(quality_score * 100, 0, 100))
            
        except Exception as e:
            self.logger.error(f"Quality assessment failed: {e}")
            return 50.0  # Default medium quality

class ImprovedUltraQualityEnhancer:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏û‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.target_face_size = 224  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 160 ‡πÄ‡∏õ‡πá‡∏ô 224
        self.use_lab_colorspace = True  # ‡πÉ‡∏ä‡πâ LAB color space
        
    def enhance_image_ultra_quality(self, image: np.ndarray) -> np.ndarray:
        """Ultra Quality Enhancement with LAB Color Space"""
        try:
            original_height, original_width = image.shape[:2]
            self.logger.debug(f"Original size: {original_width}x{original_height}")
            
            # === STAGE 1: Color Space Optimization ===
            if self.use_lab_colorspace:
                # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô LAB ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
                lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
                work_image = lab_image.copy()
                self.logger.debug("Using LAB color space for enhancement")
            else:
                work_image = image.copy()
            
            # === STAGE 2: Advanced Noise Reduction ===
            if self.use_lab_colorspace:
                # ‡∏•‡∏î noise ‡πÉ‡∏ô L channel ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                work_image[:, :, 0] = cv2.fastNlMeansDenoising(
                    work_image[:, :, 0], None, 10, 7, 21
                )
            else:
                work_image = cv2.fastNlMeansDenoisingColored(
                    work_image, None, 10, 10, 7, 21
                )
            
            # === STAGE 3: Optimized CLAHE ===
            if self.use_lab_colorspace:
                # ‡πÉ‡∏ä‡πâ CLAHE ‡∏Å‡∏±‡∏ö L channel ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
                work_image[:, :, 0] = clahe.apply(work_image[:, :, 0])
                # ‡πÅ‡∏õ‡∏•‡∏á‡∏Å‡∏•‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô BGR
                enhanced = cv2.cvtColor(work_image, cv2.COLOR_LAB2BGR)
            else:
                # ‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏î‡∏¥‡∏°
                lab = cv2.cvtColor(work_image, cv2.COLOR_BGR2LAB)
                clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))
                lab[:, :, 0] = clahe.apply(lab[:, :, 0])
                enhanced = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
            
            # === STAGE 4: Advanced Gamma Correction ===
            enhanced = self.adaptive_gamma_correction_v2(enhanced)
            
            # === STAGE 5: Bilateral Filter (edge-preserving smoothing) ===
            enhanced = cv2.bilateralFilter(enhanced, 9, 75, 75)
            
            # === STAGE 6: Color Enhancement ===
            hsv = cv2.cvtColor(enhanced, cv2.COLOR_BGR2HSV)
            hsv[:, :, 1] = cv2.multiply(hsv[:, :, 1], 1.2)  # ‡πÄ‡∏û‡∏¥‡πà‡∏° saturation 20%
            enhanced = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
            
            # === STAGE 7: Final Contrast Enhancement ===
            enhanced = cv2.convertScaleAbs(enhanced, alpha=1.1, beta=10)
            
            return enhanced
            
        except Exception as e:
            self.logger.error(f"Enhanced processing failed: {e}")
            return image  # ‡∏ñ‡πâ‡∏≤‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö
    
    def adaptive_gamma_correction_v2(self, image: np.ndarray) -> np.ndarray:
        """‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á gamma correction ‡πÉ‡∏´‡πâ‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏∂‡πâ‡∏ô"""
        try:
            # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô LAB ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå brightness
            lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
            l_channel = lab[:, :, 0]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì histogram ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ gamma ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            hist = cv2.calcHist([l_channel], [0], None, [256], [0, 256])
            hist_norm = hist.ravel() / hist.sum()
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cumulative distribution
            cdf = hist_norm.cumsum()
            
            # ‡∏´‡∏≤ gamma ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏à‡∏≤‡∏Å CDF
            median_val = np.where(cdf >= 0.5)[0][0]
            
            if median_val < 85:  # ‡∏†‡∏≤‡∏û‡∏°‡∏∑‡∏î
                gamma = 0.6
            elif median_val > 170:  # ‡∏†‡∏≤‡∏û‡∏™‡∏ß‡πà‡∏≤‡∏á
                gamma = 1.4
            else:
                gamma = 1.0
                
            # Apply gamma correction
            inv_gamma = 1.0 / gamma
            table = np.array([((i / 255.0) ** inv_gamma) * 255 
                            for i in np.arange(0, 256)]).astype("uint8")
            corrected = cv2.LUT(image, table)
            
            return corrected
            
        except Exception:
            return image
    
    def crop_face_ultra_quality_v2(self, image: np.ndarray, bbox, target_size: int = None) -> np.ndarray:
        """Ultra quality face cropping with improved sizing strategy"""
        try:
            if target_size is None:
                target_size = self.target_face_size
                
            height, width = image.shape[:2]
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì face center
            face_center_x = (bbox.x1 + bbox.x2) / 2
            face_center_y = (bbox.y1 + bbox.y2) / 2
            face_width = bbox.x2 - bbox.x1
            face_height = bbox.y2 - bbox.y1
            
            # ‡∏õ‡∏£‡∏±‡∏ö margin ‡πÅ‡∏ö‡∏ö adaptive ‡∏ï‡∏≤‡∏°‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            face_size = min(face_width, face_height)
            if face_size < 64:
                margin_factor = 0.5  # margin ‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏•‡πá‡∏Å
            elif face_size < 128:
                margin_factor = 0.4
            else:
                margin_factor = 0.3
                
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î crop ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
            crop_size = max(face_width, face_height) * (1 + margin_factor)
            
            # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì crop coordinates
            x1 = max(0, int(face_center_x - crop_size / 2))
            y1 = max(0, int(face_center_y - crop_size / 2))
            x2 = min(width, int(face_center_x + crop_size / 2))
            y2 = min(height, int(face_center_y + crop_size / 2))
            
            # Crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            face_crop = image[y1:y2, x1:x2]
            
            if face_crop.size == 0:
                return np.array([])
            
            # ‡πÉ‡∏ä‡πâ INTER_LANCZOS4 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            if face_crop.shape[0] != target_size or face_crop.shape[1] != target_size:
                face_crop = cv2.resize(face_crop, (target_size, target_size), 
                                     interpolation=cv2.INTER_LANCZOS4)
            
            return face_crop
            
        except Exception as e:
            self.logger.error(f"Ultra quality cropping v2 failed: {e}")
            return np.array([])
    
    def assess_image_quality_v2(self, image: np.ndarray) -> Dict[str, float]:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # 1. Sharpness (Laplacian variance)
            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()
            
            # 2. Brightness
            brightness = np.mean(gray)
            
            # 3. Contrast (standard deviation)
            contrast = np.std(gray)
            
            # 4. Noise level (estimate)
            noise = np.std(gray) / (brightness + 1e-6)
            
            # 5. Edge density
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / edges.size
            
            # 6. Gradient magnitude
            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            gradient_magnitude = np.mean(np.sqrt(grad_x**2 + grad_y**2))
            
            return {
                'sharpness': float(sharpness),
                'brightness': float(brightness),
                'contrast': float(contrast),
                'noise': float(noise),
                'edge_density': float(edge_density),
                'gradient_magnitude': float(gradient_magnitude),
                'overall_quality': float((sharpness / 100 + contrast / 50 + edge_density * 100) / 3)
            }
            
        except Exception:
            return {'overall_quality': 0.5}

@dataclass
class TestResult:
    """‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏û"""
    image_path: str
    image_type: ImageType
    faces_detected: int
    faces_recognized: int
    recognition_results: List[Dict[str, Any]]
    processing_time: float
    quality_metrics: Dict[str, float]
    grafiqs_quality: float
    success: bool
    error_message: Optional[str] = None

class ComprehensiveFaceRecognitionTest:
    """‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á"""
    
    def __init__(self, test_images_dir: str = "D:/projec-final/test_images"):
        self.setup_logging()
        self.test_images_dir = Path(test_images_dir)
        self.output_dir = Path("output/comprehensive_test_results")
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # === Improved Quality Enhancer ===
        self.ultra_enhancer = ImprovedUltraQualityEnhancer()
        self.grafiqs_quality = GraFIQsQualityAssessment()
        self.logger.info("üöÄ Improved Ultra Quality Enhancement initialized")
        
        # === Enhanced Configuration ===
        # ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á 3 ‡πÇ‡∏°‡πÄ‡∏î‡∏• (FaceNet, AdaFace, ArcFace)
        self.config = RecognitionConfig(
            similarity_threshold=0.60,
            unknown_threshold=0.55,
            quality_threshold=0.2,
            preferred_model=None  # ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
        )
        
        # === Ensemble Model Weights ===
        self.model_weights = {
            'facenet': 0.30,  # 30%
            'adaface': 0.40,  # 40% 
            'arcface': 0.30   # 30%
        }
        
        # === Multi-tier Thresholds ===
        self.thresholds = {
            'high_confidence': 0.75,
            'medium_confidence': 0.65,
            'low_confidence': 0.55,
            'rejection': 0.50,
            'cross_person_gap': 0.15,
            'group_photo_penalty': 0.00,
            'grafiqs_quality_threshold': 40.0  # ‡πÄ‡∏û‡∏¥‡πà‡∏° threshold ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GraFIQs
        }
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô VRAM manager
        vram_config = {
            "reserved_vram_mb": 512,
            "model_vram_estimates": {
                "yolov9c-face": 512 * 1024 * 1024,
                "yolov9e-face": 2048 * 1024 * 1024,
                "yolov11m-face": 2 * 1024 * 1024 * 1024,
                "adaface": 89 * 1024 * 1024,
                "arcface": 249 * 1024 * 1024,
                "facenet": 249 * 1024 * 1024,
            }
        }
        self.vram_manager = VRAMManager(vram_config)
        
        # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô services
        self.face_service = FaceRecognitionService(self.config, self.vram_manager)
        
        detection_config = {
            "yolov9c_model_path": "model/face-detection/yolov9c-face-lindevs.onnx",
            "yolov9e_model_path": "model/face-detection/yolov9e-face-lindevs.onnx",
            "yolov11m_model_path": "model/face-detection/yolov11m-face.pt",
            "max_usable_faces_yolov9": 8,
            "min_agreement_ratio": 0.7,
            "min_quality_threshold": 60,
            "conf_threshold": 0.15,
            "iou_threshold": 0.4,
            "img_size": 640
        }
        self.detection_service = FaceDetectionService(self.vram_manager, detection_config)
        
        # ‡πÄ‡∏Å‡πá‡∏ö embeddings ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
        self.registered_people = {}
        
        # === Test Statistics ===
        self.test_stats = {
            'total_tests': 0,
            'successful_tests': 0,
            'total_faces_detected': 0,
            'total_faces_recognized': 0,
            'recognition_by_type': {},
            'quality_distribution': {},
            'processing_times': [],
            'error_count': 0
        }
        
        self._initialized = False
    
    def convert_numpy_types(self, obj):
        """‡πÅ‡∏õ‡∏•‡∏á numpy types ‡πÄ‡∏õ‡πá‡∏ô Python types ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£ serialization ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á"""
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: self.convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self.convert_numpy_types(item) for item in obj]
        else:
            return obj
    
    def setup_logging(self):
        """‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('comprehensive_face_test.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)

    async def initialize_services(self):
        """Initialize all services"""
        if not self._initialized:
            await self.face_service.initialize()
            await self.detection_service.initialize()
            self._initialized = True
            self.logger.info("‚úÖ Services initialized successfully")

    def get_test_images(self) -> Dict[ImageType, List[str]]:
        """‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ö‡πà‡∏á‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"""
        
        # Reference files ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô
        reference_files = [
            ("boss_01.jpg", "Boss"), ("boss_02.jpg", "Boss"), ("boss_03.jpg", "Boss"),
            ("boss_04.jpg", "Boss"), ("boss_05.jpg", "Boss"), ("boss_06.jpg", "Boss"),
            ("boss_07.jpg", "Boss"), ("boss_08.jpg", "Boss"), ("boss_09.jpg", "Boss"),
            ("boss_10.jpg", "Boss"), ("night_01.jpg", "Night"), ("night_02.jpg", "Night"),
            ("night_03.jpg", "Night"), ("night_04.jpg", "Night"), ("night_05.jpg", "Night"),
            ("night_06.jpg", "Night"), ("night_07.jpg", "Night"), ("night_08.jpg", "Night"),
            ("night_09.jpg", "Night"), ("night_10.jpg", "Night")
        ]
        
        test_images = {
            ImageType.REFERENCE: [filename for filename, _ in reference_files],
            
            ImageType.GROUP: [
                "night_group01.jpg", "night_group02.jpg", 
                "boss_group01.jpg", "boss_group02.jpg", "boss_group03.jpg"
            ],
            
            ImageType.GLASSES: [
                "boss_glass02.jpg", "boss_glass03.jpg", "boss_glass04.jpg"
            ],
            
            ImageType.FACE_SWAP: [
                "face-swap01.png", "face-swap02.png", "face-swap03.png"
            ],
            
            ImageType.SPOOFING: [
                "spoofing_01.jpg", "spoofing_02.jpg"
            ],
            
            ImageType.REGISTERED: [
                "boss_01.jpg", "boss_05.jpg", "boss_10.jpg",
                "night_01.jpg", "night_05.jpg", "night_10.jpg"
            ]
        }
        
        return test_images

    async def enroll_person_improved(self, image_path: str, person_name: str) -> bool:
        """‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏ö‡∏∏‡∏Ñ‡∏Ñ‡∏•‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î"""
        try:
            full_path = self.test_images_dir / image_path
            if not full_path.exists():
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {full_path}")
                return False
                
            # ‡∏≠‡πà‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏†‡∏≤‡∏û
            image = cv2.imread(str(full_path))
            if image is None:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ: {full_path}")
                return False
                
            # === IMPROVED ULTRA QUALITY ENHANCEMENT ===
            enhanced_image = self.ultra_enhancer.enhance_image_ultra_quality(image)
            
            # === GraFIQs Quality Assessment ===
            grafiqs_score = self.grafiqs_quality.assess_quality(enhanced_image)
            
            if grafiqs_score < self.thresholds['grafiqs_quality_threshold']:
                self.logger.warning(f"‚ö†Ô∏è Low quality image rejected: {image_path} (GraFIQs: {grafiqs_score:.1f})")
                return False
                
            self.logger.info(f"üìä GraFIQs Quality Score: {grafiqs_score:.1f}")
            
            # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
            quality_metrics = self.ultra_enhancer.assess_image_quality_v2(enhanced_image)
            self.logger.debug(f"Quality metrics: {quality_metrics}")
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            detection_result = await self.detection_service.detect_faces(enhanced_image)
            if not detection_result.faces:
                self.logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÉ‡∏ô: {image_path}")
                return False
            
            # ‡πÉ‡∏ä‡πâ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ confidence ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
            best_face = max(detection_result.faces, key=lambda f: f.bbox.confidence)
            # === IMPROVED ULTRA QUALITY FACE CROPPING ===
            face_crop = self.ultra_enhancer.crop_face_ultra_quality_v2(
                enhanced_image, best_face.bbox, target_size=224
            )
            if face_crop.size == 0:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡πÑ‡∏î‡πâ: {image_path}")
                return False
            
            # ‡∏î‡∏∂‡∏á embeddings ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            model_embeddings = await self.extract_embeddings_from_all_models(face_crop)
            
            if not model_embeddings:
                self.logger.error(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÑ‡∏î‡πâ: {image_path}")
                return False
                
            # ‡πÄ‡∏Å‡πá‡∏ö embedding
            if person_name not in self.registered_people:
                self.registered_people[person_name] = []
                
            self.registered_people[person_name].append({
                'model_embeddings': model_embeddings,  # ‡πÄ‡∏Å‡πá‡∏ö embeddings ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                'source_image': str(full_path),
                'quality': best_face.bbox.confidence,
                'grafiqs_quality': grafiqs_score,
                'quality_metrics': quality_metrics,
                'bbox': best_face.bbox,
                'enrollment_time': datetime.now().isoformat(),
                'is_reference': True
            })
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•
            model_count = len(model_embeddings)
            self.logger.info(f"‚úÖ ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {person_name} ‡∏à‡∏≤‡∏Å {image_path} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à "
                           f"(Quality: {best_face.bbox.confidence:.3f}, GraFIQs: {grafiqs_score:.1f}, "
                           f"Models: {model_count}/3)")
            return True
                
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô {image_path}: {e}")
            return False

    async def enroll_reference_images(self) -> bool:
        """‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        self.logger.info("üìù ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î...")
        
        reference_files = [
            ("boss_01.jpg", "Boss"), ("boss_02.jpg", "Boss"), ("boss_03.jpg", "Boss"),
            ("boss_04.jpg", "Boss"), ("boss_05.jpg", "Boss"), ("boss_06.jpg", "Boss"),
            ("boss_07.jpg", "Boss"), ("boss_08.jpg", "Boss"), ("boss_09.jpg", "Boss"),
            ("boss_10.jpg", "Boss"), ("night_01.jpg", "Night"), ("night_02.jpg", "Night"),
            ("night_03.jpg", "Night"), ("night_04.jpg", "Night"), ("night_05.jpg", "Night"),
            ("night_06.jpg", "Night"), ("night_07.jpg", "Night"), ("night_08.jpg", "Night"),
            ("night_09.jpg", "Night"), ("night_10.jpg", "Night")
        ]
        
        total_registered = 0
        
        for image_path, person_name in reference_files:
            if await self.enroll_person_improved(image_path, person_name):
                total_registered += 1
                
        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
        self.logger.info("=" * 60)
        self.logger.info("üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î:")
        for person_name, embeddings in self.registered_people.items():
            avg_quality = np.mean([emb['quality'] for emb in embeddings])
            avg_grafiqs = np.mean([emb['grafiqs_quality'] for emb in embeddings])
            self.logger.info(f"   üë§ {person_name}: {len(embeddings)} ‡∏†‡∏≤‡∏û "
                           f"(Avg Quality: {avg_quality:.3f}, Avg GraFIQs: {avg_grafiqs:.1f})")
        self.logger.info(f"   üìà ‡∏£‡∏ß‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_registered} ‡∏†‡∏≤‡∏û")
        self.logger.info("   üéØ Face crop size: 224x224 (Ultra Quality)")
        self.logger.info("   üî¨ Using LAB Color Space + GraFIQs Assessment")
        
        return total_registered > 0

    async def test_single_image(self, image_path: str, image_type: ImageType) -> TestResult:
        """‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ö"""
        start_time = time.time()
        
        try:
            full_path = self.test_images_dir / image_path
            
            if not full_path.exists():
                return TestResult(
                    image_path=image_path,
                    image_type=image_type,
                    faces_detected=0,
                    faces_recognized=0,
                    recognition_results=[],
                    processing_time=0,
                    quality_metrics={},
                    grafiqs_quality=0,
                    success=False,
                    error_message=f"File not found: {full_path}"
                )
            
            # ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û
            image = cv2.imread(str(full_path))
            if image is None:
                return TestResult(
                    image_path=image_path,
                    image_type=image_type,
                    faces_detected=0,
                    faces_recognized=0,
                    recognition_results=[],
                    processing_time=0,
                    quality_metrics={},
                    grafiqs_quality=0,
                    success=False,
                    error_message="Cannot read image"
                )
            
            # === IMPROVED ULTRA QUALITY ENHANCEMENT ===
            enhanced_image = self.ultra_enhancer.enhance_image_ultra_quality(image)
            
            # === GraFIQs Quality Assessment ===
            grafiqs_quality = self.grafiqs_quality.assess_quality(enhanced_image)
            
            # ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
            quality_metrics = self.ultra_enhancer.assess_image_quality_v2(enhanced_image)
            
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
            detection_result = await self.detection_service.detect_faces(enhanced_image)
            
            recognition_results = []
            faces_recognized = 0
            
            if detection_result.faces:
                for i, face in enumerate(detection_result.faces):
                    # === IMPROVED ULTRA QUALITY FACE CROPPING ===
                    face_crop = self.ultra_enhancer.crop_face_ultra_quality_v2(
                        enhanced_image, face.bbox, target_size=224
                    )
                    if face_crop.size > 0:
                        # ‡∏î‡∏∂‡∏á embeddings ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                        model_embeddings = await self.extract_embeddings_from_all_models(face_crop)
                        
                        if model_embeddings:
                            # ‡∏à‡∏î‡∏à‡∏≥‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢ ensemble
                            best_match = await self.advanced_face_matching(
                                model_embeddings, image_type
                            )
                            
                            result = {
                                'face_index': i,
                                'bbox': {
                                    'x': int(face.bbox.x1),
                                    'y': int(face.bbox.y1),
                                    'width': int(face.bbox.x2 - face.bbox.x1),
                                    'height': int(face.bbox.y2 - face.bbox.y1)
                                },
                                'detection_confidence': face.bbox.confidence,
                                'person_name': best_match['person_name'] if best_match else 'unknown',
                                'recognition_confidence': best_match['confidence'] if best_match else 0.0,
                                'grafiqs_quality': grafiqs_quality,
                                'quality_metrics': quality_metrics,
                                'model_results': best_match['model_results'] if best_match else {}
                            }
                            
                            if best_match:
                                faces_recognized += 1
                                
                            recognition_results.append(result)
            
            processing_time = time.time() - start_time
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            await self.save_result_image(enhanced_image, recognition_results, image_path, image_type)
            
            return TestResult(
                image_path=image_path,
                image_type=image_type,
                faces_detected=len(detection_result.faces) if detection_result.faces else 0,
                faces_recognized=faces_recognized,
                recognition_results=recognition_results,
                processing_time=processing_time,
                quality_metrics=quality_metrics,
                grafiqs_quality=grafiqs_quality,
                success=True
            )
            
        except Exception as e:
            processing_time = time.time() - start_time
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö {image_path}: {e}")
            
            return TestResult(
                image_path=image_path,
                image_type=image_type,
                faces_detected=0,
                faces_recognized=0,
                recognition_results=[],
                processing_time=processing_time,
                quality_metrics={},
                grafiqs_quality=0,                success=False,
                error_message=str(e)
            )
            
    async def advanced_face_matching(self, model_embeddings: Dict[str, np.ndarray], 
                                   image_type: ImageType) -> Optional[Dict[str, Any]]:
        """Advanced face matching ‡∏î‡πâ‡∏ß‡∏¢ ensemble ‡πÅ‡∏•‡∏∞ context-aware threshold"""
        if not model_embeddings:
            return None
            
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì similarity ‡πÅ‡∏¢‡∏Å‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model_similarities = {}
        
        for model_name, target_embedding in model_embeddings.items():
            person_similarities = {}
            
            for person_name, embeddings_data in self.registered_people.items():
                max_similarities = []
                
                for embedding_data in embeddings_data:
                    if 'model_embeddings' in embedding_data and model_name in embedding_data['model_embeddings']:
                        ref_embedding = embedding_data['model_embeddings'][model_name]
                        similarity = np.dot(target_embedding, ref_embedding) / (
                            np.linalg.norm(target_embedding) * np.linalg.norm(ref_embedding) + 1e-7
                        )
                        max_similarities.append(similarity)
                
                if max_similarities:
                    person_similarities[person_name] = max(max_similarities)
            
            if person_similarities:
                model_similarities[model_name] = person_similarities
        
        # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢ weighted ensemble
        ensemble_similarities = {}
        
        for person_name in self.registered_people.keys():
            weighted_sum = 0.0
            total_weight = 0.0
            
            for model_name, weight in self.model_weights.items():
                if model_name in model_similarities and person_name in model_similarities[model_name]:
                    weighted_sum += model_similarities[model_name][person_name] * weight
                    total_weight += weight
            
            if total_weight > 0:
                ensemble_similarities[person_name] = weighted_sum / total_weight
        
        if not ensemble_similarities:
            return None
        
        # ‡∏´‡∏≤‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ similarity ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        best_person = max(ensemble_similarities.keys(), 
                         key=lambda x: ensemble_similarities[x])
        best_similarity = ensemble_similarities[best_person]
        
        # ‡∏´‡∏≤‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏°‡∏µ similarity ‡∏™‡∏π‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö 2
        second_best_similarity = 0.0
        if len(ensemble_similarities) > 1:
            similarities_list = [(person, similarity) 
                               for person, similarity in ensemble_similarities.items()]
            similarities_list.sort(key=lambda x: x[1], reverse=True)
            if len(similarities_list) > 1:
                second_best_similarity = similarities_list[1][1]
        
        # Cross-person validation
        similarity_gap = best_similarity - second_best_similarity
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å threshold ‡∏ï‡∏≤‡∏° context
        if image_type == ImageType.REGISTERED:
            required_threshold = self.thresholds['medium_confidence']
        elif image_type == ImageType.FACE_SWAP:
            required_threshold = self.thresholds['high_confidence']
        elif image_type == ImageType.GROUP:
            required_threshold = self.thresholds['low_confidence']
        elif image_type == ImageType.SPOOFING:
            required_threshold = self.thresholds['high_confidence']
        else:
            required_threshold = self.thresholds['low_confidence']
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç
        if best_similarity < required_threshold:
            return None
            
        if similarity_gap < self.thresholds['cross_person_gap']:
            return None
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model_results = {}
        for model_name in model_similarities:
            if best_person in model_similarities[model_name]:
                model_results[model_name] = float(model_similarities[model_name][best_person])
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô return ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ numpy types
        result = {
            'person_name': best_person,
            'confidence': float(best_similarity),
            'similarity_gap': float(similarity_gap),
            'threshold_used': float(required_threshold),
            'image_type': image_type.value,
            'model_results': model_results  # ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô float ‡πÅ‡∏•‡πâ‡∏ß
        }
        
        # ‡πÅ‡∏õ‡∏•‡∏á numpy types ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏õ‡πá‡∏ô Python types ‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡∏Ñ‡∏∑‡∏ô
        return self.convert_numpy_types(result)

    async def save_result_image(self, image: np.ndarray, recognition_results: List[Dict], 
                              image_path: str, image_type: ImageType):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå"""
        try:
            result_image = image.copy()
            
            for result in recognition_results:
                bbox = result['bbox']
                person_name = result['person_name']
                confidence = result['recognition_confidence']
                
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏µ‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
                if person_name == 'unknown':
                    color = (0, 0, 255)  # ‡πÅ‡∏î‡∏á
                    label = "UNKNOWN"
                elif person_name in ['Boss', 'Night']:
                    if confidence > 0.75:
                        color = (0, 200, 0)  # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡πÄ‡∏Ç‡πâ‡∏°
                    else:
                        color = (0, 255, 0)  # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ß‡∏õ‡∏Å‡∏ï‡∏¥
                    label = f"{person_name.upper()}"
                else:
                    color = (255, 0, 0)  # ‡∏ô‡πâ‡∏≥‡πÄ‡∏á‡∏¥‡∏ô
                    label = f"{person_name.upper()}"
                
                # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≠‡∏ö
                cv2.rectangle(
                    result_image,
                    (bbox['x'], bbox['y']),
                    (bbox['x'] + bbox['width'], bbox['y'] + bbox['height']),
                    color, 3
                )
                
                # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                if person_name != 'unknown':
                    text = f"{label} ({confidence:.1%})"
                else:
                    text = label
                    
                # ‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                font_scale = 0.6
                thickness = 2
                (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, thickness)
                
                # ‡∏ß‡∏≤‡∏î‡∏û‡∏∑‡πâ‡∏ô‡∏´‡∏•‡∏±‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                cv2.rectangle(
                    result_image,
                    (bbox['x'], bbox['y'] - text_height - 10),
                    (bbox['x'] + text_width, bbox['y']),
                    color, -1
                )
                
                # ‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
                cv2.putText(
                    result_image,
                    text,
                    (bbox['x'], bbox['y'] - 5),
                    cv2.FONT_HERSHEY_SIMPLEX,
                    font_scale,
                    (255, 255, 255),
                    thickness
                )
            
            # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
            output_filename = f"result_{image_type.value}_{Path(image_path).stem}.jpg"
            output_path = self.output_dir / output_filename
            cv2.imwrite(str(output_path), result_image)
            
        except Exception as e:
            self.logger.error(f"‚ùå Error saving result image: {e}")

    async def run_comprehensive_test(self):
        """‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        try:
            self.logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏£‡∏∞‡∏ö‡∏ö Face Recognition")
            self.logger.info("=" * 80)
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: Initialize services
            await self.initialize_services()
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á
            if not await self.enroll_reference_images():
                self.logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡πÑ‡∏î‡πâ")
                return
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 3: ‡∏î‡∏∂‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö
            test_images = self.get_test_images()
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 4: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏†‡∏≤‡∏û
            all_results = []
            total_start_time = time.time()
            
            for image_type, image_list in test_images.items():
                if image_type == ImageType.REFERENCE:
                    continue  # ‡∏Ç‡πâ‡∏≤‡∏° reference ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡πâ‡∏ß
                    
                self.logger.info(f"\nüß™ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö {image_type.value.upper()} Images...")
                self.logger.info("-" * 60)
                
                type_results = []
                for image_path in image_list:
                    self.logger.info(f"üîç ‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {image_path}")
                    result = await self.test_single_image(image_path, image_type)
                    type_results.append(result)
                    all_results.append(result)
                    
                    # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥
                    self.test_stats['total_tests'] += 1
                    if result.success:
                        self.test_stats['successful_tests'] += 1
                        self.test_stats['total_faces_detected'] += result.faces_detected
                        self.test_stats['total_faces_recognized'] += result.faces_recognized
                        self.test_stats['processing_times'].append(result.processing_time)
                    else:
                        self.test_stats['error_count'] += 1
                    
                    # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
                    if image_type.value not in self.test_stats['recognition_by_type']:
                        self.test_stats['recognition_by_type'][image_type.value] = {
                            'total': 0, 'detected': 0, 'recognized': 0
                        }
                    
                    self.test_stats['recognition_by_type'][image_type.value]['total'] += 1
                    self.test_stats['recognition_by_type'][image_type.value]['detected'] += result.faces_detected
                    self.test_stats['recognition_by_type'][image_type.value]['recognized'] += result.faces_recognized
                
                # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ô‡∏µ‡πâ
                self.summarize_type_results(image_type, type_results)
            
            total_time = time.time() - total_start_time
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 5: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•
            report_path = await self.generate_comprehensive_report(all_results, total_time)
            
            # ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 6: ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.display_final_summary(total_time, report_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏£‡πâ‡∏≤‡∏¢‡πÅ‡∏£‡∏á: {e}")
            import traceback
            traceback.print_exc()

    def summarize_type_results(self, image_type: ImageType, results: List[TestResult]):
        """‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó"""
        if not results:
            return
            
        total_images = len(results)
        successful_tests = len([r for r in results if r.success])
        total_faces_detected = sum(r.faces_detected for r in results)
        total_faces_recognized = sum(r.faces_recognized for r in results)
        avg_processing_time = np.mean([r.processing_time for r in results if r.success])
        avg_grafiqs_quality = np.mean([r.grafiqs_quality for r in results if r.success])
        
        recognition_rate = (total_faces_recognized / total_faces_detected * 100) if total_faces_detected > 0 else 0
        
        self.logger.info(f"üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• {image_type.value.upper()}:")
        self.logger.info(f"   üìÅ ‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_images}")
        self.logger.info(f"   ‚úÖ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {successful_tests}")
        self.logger.info(f"   üë• ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: {total_faces_detected} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üéØ ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ: {total_faces_recognized} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üìà ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥: {recognition_rate:.1f}%")
        self.logger.info(f"   ‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_processing_time:.3f}s")
        self.logger.info(f"   üî¨ GraFIQs ‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_grafiqs_quality:.1f}")

    async def generate_comprehensive_report(self, results: List[TestResult], total_time: float) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏°
        total_tests = len(results)
        successful_tests = len([r for r in results if r.success])
        total_faces_detected = sum(r.faces_detected for r in results)
        total_faces_recognized = sum(r.faces_recognized for r in results)
        overall_recognition_rate = (total_faces_recognized / total_faces_detected * 100) if total_faces_detected > 0 else 0
        
        processing_times = [r.processing_time for r in results if r.success]
        avg_processing_time = np.mean(processing_times) if processing_times else 0
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô JSON
        report_data = {
            'test_metadata': {
                'timestamp': datetime.now().isoformat(),
                'test_duration': total_time,
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'error_count': total_tests - successful_tests,
                'success_rate': (successful_tests / total_tests * 100) if total_tests > 0 else 0
            },
            'recognition_performance': {
                'total_faces_detected': total_faces_detected,
                'total_faces_recognized': total_faces_recognized,
                'overall_recognition_rate': overall_recognition_rate,
                'average_processing_time': avg_processing_time,
                'fps': 1.0 / avg_processing_time if avg_processing_time > 0 else 0
            },
            'recognition_by_type': self.test_stats['recognition_by_type'],
            'quality_analysis': {
                'avg_grafiqs_scores': {},
                'quality_distribution': {}
            },
            'system_improvements': {
                'lab_colorspace_enabled': True,
                'grafiqs_quality_assessment': True,
                'improved_face_cropping': True,
                'target_face_size': 224,
                'quality_threshold': self.thresholds['grafiqs_quality_threshold']
            },
            'detailed_results': []
        }
        
        # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
        for image_type in ImageType:
            if image_type == ImageType.REFERENCE:
                continue
                
            type_results = [r for r in results if r.image_type == image_type]
            if type_results:
                grafiqs_scores = [r.grafiqs_quality for r in type_results if r.success]
                if grafiqs_scores:
                    report_data['quality_analysis']['avg_grafiqs_scores'][image_type.value] = np.mean(grafiqs_scores)
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏†‡∏≤‡∏û
        for result in results:
            result_dict = {
                'image_path': result.image_path,
                'image_type': result.image_type.value,
                'faces_detected': result.faces_detected,
                'faces_recognized': result.faces_recognized,
                'processing_time': result.processing_time,
                'grafiqs_quality': result.grafiqs_quality,
                'success': result.success,
                'recognition_results': result.recognition_results
            }
            
            if result.error_message:
                result_dict['error_message'] = result.error_message
                
            report_data['detailed_results'].append(result_dict)
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô JSON
        json_filename = f"comprehensive_test_report_{timestamp}.json"
        json_path = self.output_dir / json_filename
        
        # ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô Python types ‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
        report_data_converted = self.convert_numpy_types(report_data)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data_converted, f, ensure_ascii=False, indent=2)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Markdown
        md_filename = f"comprehensive_test_report_{timestamp}.md"
        md_path = self.output_dir / md_filename
        
        md_content = f"""# ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö Face Recognition ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°

## üöÄ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

**‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏î‡∏™‡∏≠‡∏ö:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {total_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ  
**‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** LAB Color Space + GraFIQs Quality Assessment + Improved Face Cropping

## üìä ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

### ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°
- **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {total_tests} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á
- **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:** {successful_tests} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á ({successful_tests/total_tests*100:.1f}%)
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö:** {total_faces_detected} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ:** {total_faces_recognized} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- **‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡∏£‡∏ß‡∏°:** {overall_recognition_rate:.1f}%
- **‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢:** {avg_processing_time:.3f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
- **‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•:** {1.0/avg_processing_time if avg_processing_time > 0 else 0:.1f} FPS

### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö
‚úÖ **LAB Color Space Enhancement** - ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏™‡∏á  
‚úÖ **GraFIQs Quality Assessment** - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•  
‚úÖ **Improved Face Cropping** - ‡∏Å‡∏≤‡∏£ crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î 224x224 ‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•  
‚úÖ **Advanced Gamma Correction** - ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏á‡πÅ‡∏ö‡∏ö adaptive  
‚úÖ **LANCZOS4 Interpolation** - ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á

## üìà ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏†‡∏≤‡∏û

"""
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó
        for image_type, stats in report_data['recognition_by_type'].items():
            if stats['total'] > 0:
                recognition_rate = (stats['recognized'] / stats['detected'] * 100) if stats['detected'] > 0 else 0
                detection_rate = (stats['detected'] / stats['total'])
                
                md_content += f"""### {image_type.upper()} Images
- **‡∏†‡∏≤‡∏û‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:** {stats['total']} ‡∏†‡∏≤‡∏û
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö:** {stats['detected']} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ {detection_rate:.1f} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤/‡∏†‡∏≤‡∏û)
- **‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ:** {stats['recognized']} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤
- **‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥:** {recognition_rate:.1f}%

"""
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û
        md_content += """## üî¨ ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û (GraFIQs)

"""
        
        for image_type, avg_score in report_data['quality_analysis']['avg_grafiqs_scores'].items():
            md_content += f"- **{image_type.upper()}:** {avg_score:.1f}/100\n"
        
        md_content += f"""

## üéØ ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°

‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô:

1. **‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ LAB Color Space** ‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏î‡πâ‡∏≤‡∏ô‡πÅ‡∏™‡∏á
2. **GraFIQs Quality Assessment** ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏£‡∏≠‡∏á‡∏†‡∏≤‡∏û‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≥‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
3. **‡∏Å‡∏≤‡∏£ Crop ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î 224x224** ‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
4. **LANCZOS4 Interpolation** ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
5. **Advanced Gamma Correction** ‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏á‡πÅ‡∏ö‡∏ö adaptive ‡∏ï‡∏≤‡∏°‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏û

## üí° ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ï‡πà‡∏≠

### ‡∏£‡∏∞‡∏¢‡∏∞‡∏™‡∏±‡πâ‡∏ô (1-2 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå)
- ‡πÄ‡∏û‡∏¥‡πà‡∏° Real-ESRGAN ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ï‡πà‡∏≥
- ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á threshold ‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö

### ‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏•‡∏≤‡∏á (1-2 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)  
- ‡πÄ‡∏û‡∏¥‡πà‡∏° Multi-image enrollment
- ‡∏û‡∏±‡∏í‡∏ô‡∏≤ GPU memory pooling

### ‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß (3-6 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô)
- ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ Foundation Models (DINOv2, Vision Transformers)
- ‡∏û‡∏±‡∏í‡∏ô‡∏≤ Ensemble Quality Assessment

---
*‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏î‡∏¢ Comprehensive Face Recognition Test System v2.0*  
*‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏õ‡∏µ 2023-2025*
"""
        
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        self.logger.info(f"üìÑ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°: {json_path}")
        self.logger.info(f"üìÑ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Markdown: {md_path}")
        
        return str(md_path)

    def display_final_summary(self, total_time: float, report_path: str):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        self.logger.info("=" * 80)
        self.logger.info("üéâ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏£‡∏∞‡∏ö‡∏ö Face Recognition")
        self.logger.info("=" * 80)
        
        # ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô
        total_tests = self.test_stats['total_tests']
        successful_tests = self.test_stats['successful_tests']
        total_faces_detected = self.test_stats['total_faces_detected']
        total_faces_recognized = self.test_stats['total_faces_recognized']
        
        overall_recognition_rate = (total_faces_recognized / total_faces_detected * 100) if total_faces_detected > 0 else 0
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        avg_processing_time = np.mean(self.test_stats['processing_times']) if self.test_stats['processing_times'] else 0
        
        self.logger.info("üìä **‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏£‡∏ß‡∏°:**")
        self.logger.info(f"   üß™ ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_tests} ‡∏Ñ‡∏£‡∏±‡πâ‡∏á")
        self.logger.info(f"   ‚úÖ ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {success_rate:.1f}% ({successful_tests}/{total_tests})")
        self.logger.info(f"   üë• ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏û‡∏ö: {total_faces_detected} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üéØ ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏î‡πâ: {total_faces_recognized} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤")
        self.logger.info(f"   üìà ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏à‡∏î‡∏à‡∏≥‡∏£‡∏ß‡∏°: {overall_recognition_rate:.1f}%")
        self.logger.info(f"   ‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢: {avg_processing_time:.3f}s")
        self.logger.info(f"   üïê ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î: {total_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        
        self.logger.info("\nüî¨ **‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:**")
        self.logger.info("   ‚úÖ LAB Color Space Enhancement")
        self.logger.info(f"   ‚úÖ GraFIQs Quality Assessment (Threshold: {self.thresholds['grafiqs_quality_threshold']})")
        self.logger.info("   ‚úÖ Improved Face Cropping (224x224)")
        self.logger.info("   ‚úÖ Advanced Gamma Correction")
        self.logger.info("   ‚úÖ LANCZOS4 Interpolation")
        
        self.logger.info("\nüìà **‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó:**")
        for image_type, stats in self.test_stats['recognition_by_type'].items():
            if stats['total'] > 0:
                recognition_rate = (stats['recognized'] / stats['detected'] * 100) if stats['detected'] > 0 else 0
                self.logger.info(f"   üìÅ {image_type.upper()}: {recognition_rate:.1f}% "
                               f"({stats['recognized']}/{stats['detected']} ‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤ ‡∏à‡∏≤‡∏Å {stats['total']} ‡∏†‡∏≤‡∏û)")
        
        # ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°
        if overall_recognition_rate > 38.5:  # ‡∏£‡∏∞‡∏ö‡∏ö‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏î‡πâ 38.5%
            improvement = overall_recognition_rate - 38.5
            self.logger.info("\nüéâ **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!**")
            self.logger.info(f"   üìä ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥: +{improvement:.1f}% (‡∏à‡∏≤‡∏Å 38.5% ‡πÄ‡∏õ‡πá‡∏ô {overall_recognition_rate:.1f}%)")
            self.logger.info(f"   üöÄ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ó‡∏ò‡πå: {improvement/38.5*100:.1f}%")
        else:
            self.logger.info("\n‚ö†Ô∏è **‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°:**")
            self.logger.info(f"   üìä ‡∏¢‡∏±‡∏á‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: {overall_recognition_rate:.1f}% < 38.5%")
        
        self.logger.info(f"\nüìÑ **‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°:** {report_path}")
        self.logger.info(f"üóÇÔ∏è **‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå:** {self.output_dir}")
        self.logger.info("\nüöÄ **‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå!**")
        self.logger.info("üí° ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°")
        
    async def extract_embeddings_from_all_models(self, face_image: np.ndarray) -> Dict[str, Any]:
        """‡∏î‡∏∂‡∏á embeddings ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö (FaceNet, AdaFace, ArcFace)"""
        embeddings = {}
        models = [ModelType.FACENET, ModelType.ADAFACE, ModelType.ARCFACE]
        model_names = ['facenet', 'adaface', 'arcface']

        for i, model_type in enumerate(models):
            try:
                embedding = await self.face_service.extract_embedding(face_image) # Removed model_type
                if embedding:
                    embeddings[model_names[i]] = embedding.vector
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á embedding ‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• {model_names[i]}: {e}")

        # ‡πÅ‡∏õ‡∏•‡∏á numpy types ‡πÄ‡∏õ‡πá‡∏ô Python types ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ JSON serialization
        return self.convert_numpy_types(embeddings)


async def main():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏´‡∏•‡∏±‡∏Å"""
    try:
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
        test_system = ComprehensiveFaceRecognitionTest()
        
        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
        await test_system.run_comprehensive_test()
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ñ‡∏π‡∏Å‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ")
    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())